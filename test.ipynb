{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = xml_processor.events['Name'].values\n",
    "unique_events = np.unique(events)\n",
    "print(unique_events)\n",
    "\n",
    "print(psg_processor.start_datetime)\n",
    "print(psg_processor.end_datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_type = 'Obstructive Apnea'\n",
    "event_df = df_sleep_events[df_sleep_events['Name'].str.contains(event_type)]\n",
    "event_df\n",
    "\n",
    "# 2024-06-21 03:33:36.600\t2024-06-21 03:33:54.830\n",
    "start_datetime = datetime(2024, 6, 20, 22, 2, 35)\n",
    "end_datetime = datetime(2024, 6, 21, 5, 54, 58) \n",
    "\n",
    "filtered_df = df.loc[start_datetime:end_datetime]\n",
    "# Make a df subset with all the columns except: \n",
    "# 'ECG', 'Thor', 'Movement Time (MT)', 'Wakefulness (W)', 'NREM Sleep Stage 1 (N1)', \n",
    "# 'REM Sleep', 'NREM Sleep Stage 2 (N2)', 'NREM Sleep Stage 3 (N3)'\n",
    "columns_to_drop = ['Movement Time (MT)', 'Wakefulness (W)', 'NREM Sleep Stage 1 (N1)', 'REM Sleep', 'NREM Sleep Stage 2 (N2)', 'NREM Sleep Stage 3 (N3)']\n",
    "df_subset = filtered_df.drop(columns=columns_to_drop)\n",
    "print(df_subset.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the dictionary to hold the 3-level structure\n",
    "structured_data = {}\n",
    "\n",
    "# Ensure the 'Start' and 'End' times in xml_processor.events are in datetime format\n",
    "# Assuming 'Start' and 'End' are already in datetime or timedelta format\n",
    "events_df = xml_processor.events\n",
    "\n",
    "# Iterate over each unique event type\n",
    "for event_type in events_df['Name'].unique():\n",
    "    # Filter the events of this type\n",
    "    event_occurrences = events_df[events_df['Name'] == event_type]\n",
    "    \n",
    "    # Initialize a list to hold all occurrences for this event type\n",
    "    occurrences_list = []\n",
    "    \n",
    "    # Iterate over each occurrence of this event type\n",
    "    for _, event_row in event_occurrences.iterrows():\n",
    "        start_time = event_row['Start']\n",
    "        end_time = event_row['End']\n",
    "        \n",
    "        # Extract the relevant rows from df_subset for ECG and Thor within the time range\n",
    "        relevant_data = df_subset[(df_subset.index >= start_time) & (df_subset.index <= end_time)][['ECG', 'Thor']]\n",
    "        \n",
    "        # Add the relevant data for this occurrence\n",
    "        occurrences_list.append({\n",
    "            'Start': start_time,\n",
    "            'End': end_time,\n",
    "            'ECG': relevant_data['ECG'].values,\n",
    "            'Thor': relevant_data['Thor'].values,\n",
    "            'Time': relevant_data.index\n",
    "        })\n",
    "    \n",
    "    # Store the list of occurrences for this event type\n",
    "    structured_data[event_type] = occurrences_list\n",
    "\n",
    "# Example: Access data for a specific event type\n",
    "# structured_data['Mixed Apnea'] will give you the occurrences and corresponding ECG/Thor data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_event_data(event_type):\n",
    "    # Check if the event_type exists in the structured_data\n",
    "    if event_type not in structured_data:\n",
    "        print(f\"Event type '{event_type}' not found in structured data.\")\n",
    "        return\n",
    "    \n",
    "    # Retrieve all occurrences for the given event type\n",
    "    event_data = structured_data[event_type]\n",
    "    \n",
    "    # Iterate over each occurrence and plot ECG and Thor data\n",
    "    for i, occurrence in enumerate(event_data):\n",
    "        # Create subplots for ECG and Thor\n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(18, 4))  # 2 rows, 1 column\n",
    "\n",
    "        # Plot ECG\n",
    "        ax1.plot(occurrence['Time'], occurrence['ECG'], label='ECG')\n",
    "        ax1.set_title(f'{event_type} - ECG (# {i + 1})')\n",
    "        ax1.set_xlabel('Time')\n",
    "        ax1.set_ylabel('ECG Value')\n",
    "        ax1.legend()\n",
    "\n",
    "        # Plot Thor\n",
    "        ax2.plot(occurrence['Time'], occurrence['Thor'], label='Thor')\n",
    "        ax2.set_title(f'{event_type} - Thor (# {i + 1})')\n",
    "        ax2.set_xlabel('Time')\n",
    "        ax2.set_ylabel('Thor Value')\n",
    "        ax2.legend()\n",
    "\n",
    "        plt.tight_layout()  # Adjust spacing between subplots\n",
    "        plt.show()  # Display the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "event_type_name = \"Central Apnea\"  # Replace with the event type you want to plot\n",
    "plot_event_data(event_type_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sleep_stages = xml_processor.sleep_stages\n",
    "\n",
    "start_datetime = psg_processor.start_datetime\n",
    "end_datetime = psg_processor.end_datetime\n",
    "\n",
    "filtered_df = df.loc[start_datetime:end_datetime]\n",
    "df_subset = filtered_df[['ECG', 'Thor']]\n",
    "print(df_subset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_sleep_stage_column(df_subset, df_sleep_stages):\n",
    "    # Create a new column in df_subset to store the sleep stage code\n",
    "    df_subset['Stage Code'] = None\n",
    "\n",
    "    # Iterate over each row in df_sleep_stages\n",
    "    for index, row in df_sleep_stages.iterrows():\n",
    "        stage_start_time = row['Start Time']\n",
    "        stage_end_time = stage_start_time + timedelta(seconds=30)\n",
    "\n",
    "        # Find the rows in df_subset that fall within the current sleep stage interval\n",
    "        mask = (df_subset.index >= stage_start_time) & (df_subset.index < stage_end_time)\n",
    "        \n",
    "        # Assign the sleep stage code to the corresponding rows in df_subset\n",
    "        df_subset.loc[mask, 'Stage Code'] = row['Sleep Stage Code']\n",
    "        # df_subset.loc[mask, 'Sleep Stage'] = row['Sleep Stage']\n",
    "        # df_subset.loc[mask, 'Time (seconds)'] = row['Time (seconds)']\n",
    "        # df_subset.loc[mask, 'Duration'] = row['Duration']\n",
    "    \n",
    "    return df_subset\n",
    "\n",
    "# Example usage\n",
    "# Apply the function to add the sleep stage column\n",
    "df_subset_with_sleep_stage = add_sleep_stage_column(df_subset, df_sleep_stages)\n",
    "\n",
    "print(df_subset_with_sleep_stage)\n",
    "print(df_subset_with_sleep_stage['Stage Code'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events1 = nk.events_find(df_subset_with_sleep_stage['Stage Code'], threshold=0, threshold_keep='below')\n",
    "events2 = nk.events_find(df_subset_with_sleep_stage['Stage Code'], threshold=0, threshold_keep='above')\n",
    "\n",
    "\n",
    "# Combine the arrays\n",
    "combined_onset = np.concatenate((events1['onset'], events2['onset']))\n",
    "combined_duration = np.concatenate((events1['duration'], events2['duration']))\n",
    "\n",
    "# Sort the arrays based on the combined onset values\n",
    "sorted_indices = np.argsort(combined_onset)\n",
    "sorted_onset = combined_onset[sorted_indices]\n",
    "sorted_duration = combined_duration[sorted_indices]\n",
    "sorted_label = [str(i+1) for i in range(len(sorted_onset))]\n",
    "\n",
    "\n",
    "# Create the merged dictionary\n",
    "merged_events = {\n",
    "    'onset': sorted_onset,\n",
    "    'duration': sorted_duration,\n",
    "    'label': sorted_label\n",
    "}\n",
    "\n",
    "print(merged_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the signal\n",
    "data_clean, info = nk.bio_process(ecg=df_subset_with_sleep_stage[\"ECG\"], \n",
    "                                  rsp=df_subset_with_sleep_stage[\"Thor\"],\n",
    "                                  keep=df_subset_with_sleep_stage[\"Stage Code\"],  \n",
    "                                  sampling_rate=1024)\n",
    "\n",
    "# Visualize some of the channels\n",
    "data_clean[[\"ECG_Rate\", \"RSP_Rate\", \"Stage Code\"]].plot(subplots=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = nk.events_plot(merged_events,data_clean[[\"ECG_Rate\", \"RSP_Rate\", \"Stage Code\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = nk.epochs_create(data_clean, merged_events, sampling_rate=1024, epochs_start=0, epochs_end='from_events')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nk.rsp_intervalrelated(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nk.ecg_intervalrelated(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = nk.epochs_create(data_clean, merged_events, sampling_rate=1024, epochs_start=-30, epochs_end=30)\n",
    "# Iterate through epoch data\n",
    "for epoch in epochs.values():\n",
    "    # Plot scaled signals, \"Stage Code\"\n",
    "    nk.signal_plot(epoch[['ECG_Rate', 'RSP_Rate']], \n",
    "                   title=epoch['Label'].values[0],  # Extract condition name\n",
    "                   standardize=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main.py\n",
    "\n",
    "from cw_radar import *\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# Example usage of CWDataProcessor\n",
    "file_path = \"/opt/data/private/ZhouWenren/SleepLab/cw_radar/radar20240620220948433561.csv\"\n",
    "sample_rate = 1000\n",
    "\n",
    "# Create an instance of the CWDataProcessor\n",
    "cw_processor = CWDataProcessor(file_path, sample_rate)\n",
    "\n",
    "# Load the data from the specified file\n",
    "cw_processor.load_data()\n",
    "print(cw_processor.data.head())\n",
    "\n",
    "# Define start and end datetime objects\n",
    "start_datetime = datetime.strptime('20240620221033', '%Y%m%d%H%M%S')\n",
    "end_datetime = datetime.strptime('20240620221133', '%Y%m%d%H%M%S')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_subset = extract_data_subset(cw_processor.data, start_datetime, end_datetime)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_sig = cw_processor.process_signal(data_subset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the processed radar data\n",
    "cw_processor.plot_processed_data(data_subset)\n",
    "\n",
    "# Frequency analysis: FFT plot\n",
    "cw_processor.plot_frequency_analysis(data_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resampled = resample_data(processed_sig, original_freq=sample_rate, target_freq=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the original dataset\n",
    "plt.figure(figsize=(20, 3))\n",
    "plt.plot(processed_sig, label='Original')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('ECG')\n",
    "plt.title('Original ECG Data')\n",
    "plt.legend()\n",
    "\n",
    "# Plot the downsampled dataset\n",
    "plt.figure(figsize=(20, 3))\n",
    "plt.plot(df_resampled, label='Downsampled')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('ECG')\n",
    "plt.title('Downsampled ECG Data')\n",
    "plt.legend()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (20, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# psg_file_path = r\"C:\\Users\\amd\\OneDrive - National University of Singapore\\SleepData\\szu_hospital\\PSG\\2024-6-20jiangyifan.edf\"\n",
    "psg_file_path = PSG_FILE_PATH\n",
    "psg_processor = PSGDataProcessor(psg_file_path)\n",
    "psg_processor.load_data()\n",
    "\n",
    "xml_file_path = XML_FILE_PATH\n",
    "start_datetime_str = \"2024-06-20 22:02:34\"\n",
    "xml_processor = XMLProcessor(xml_file_path, start_datetime_str)\n",
    "xml_processor.load()\n",
    "\n",
    "file_path = \"/opt/data/private/ZhouWenren/SleepLab/cw_radar/radar20240620220948433561.csv\"\n",
    "radar_sample_rate = 1002\n",
    "cw_processor = CWDataProcessor(file_path, radar_sample_rate)\n",
    "# Load the data from the specified file\n",
    "cw_processor.load_data()\n",
    "\n",
    "df_sleep_events = xml_processor.events\n",
    "print(psg_processor.start_datetime)\n",
    "print(xml_processor.start_datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data between two timestamps\n",
    "start_datetime = datetime(2024, 6, 20, 22, 10, 33) # Replace with your actual start datetime\n",
    "end_datetime = datetime(2024, 6, 20, 22, 11, 33)  # Replace with your actual end datetime\n",
    "print(f\"Start Timestamp: {start_datetime}, End Timestamp: {end_datetime}\")  # Print the start and end timestamps of the extracted data\n",
    "psg_date_segment = psg_processor.extract_segment_by_timestamp(start_datetime, end_datetime)\n",
    "\n",
    "radar_data_subset = extract_data_subset(cw_processor.data, start_datetime, end_datetime)\n",
    "processed_sig = cw_processor.process_signal(radar_data_subset)\n",
    "\n",
    "merged_df = integrate(psg_date_segment, xml_processor)\n",
    "print(merged_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psg_resampled = resample_data(psg_date_segment, original_freq=psg_processor.sampling_rate, target_freq=64)\n",
    "\n",
    "radar_resampled = resample_data(processed_sig, original_freq=radar_sample_rate, target_freq=64)\n",
    "radar_resampled['processed_signal'] = lowpass_filter(radar_sample_rate, radar_resampled['processed_signal'], 10, order=4)\n",
    "\n",
    "# Define a common time grid (using the minimum and maximum timestamps)\n",
    "common_time_index = pd.date_range(\n",
    "    start=max(psg_resampled.index.min(), radar_resampled.index.min()), \n",
    "    end=min(psg_resampled.index.max(), radar_resampled.index.max()), \n",
    "    freq='15.625ms'  # Adjust frequency as necessary, here it matches 64Hz\n",
    ")\n",
    "\n",
    "# Reindex and interpolate both dataframes to this common time grid\n",
    "psg_aligned = psg_resampled.reindex(common_time_index, method='nearest')\n",
    "radar_aligned = radar_resampled.reindex(common_time_index, method='nearest')\n",
    "\n",
    "# Plot the aligned signals\n",
    "plt.figure(figsize=(20, 3))\n",
    "plt.plot(psg_aligned['Thor'], label='Aligned PSG Signal')\n",
    "plt.plot(radar_aligned, label='Aligned Radar Signal')\n",
    "# plt.plot(radar_resampled, label='Aligned & Low pass Filtered Radar Signal')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Signal Amplitude')\n",
    "plt.title('Aligned Signals After Cross-Correlation')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# # Set which GPU to use (e.g., use only GPU 0)\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "# import tensorflow as tf\n",
    "\n",
    "# # Check if GPU is available\n",
    "# print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "# print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# if tf.config.list_physical_devices('GPU'):\n",
    "#     print(\"TensorFlow is using the GPU.\")\n",
    "# else:\n",
    "#     print(\"TensorFlow is not using the GPU.\")\n",
    "\n",
    "# # Example of running a simple operation on the GPU\n",
    "# with tf.device('/GPU:0'):\n",
    "#     # Create two constant tensors\n",
    "#     a = tf.constant([[1.0, 2.0], [3.0, 4.0]])\n",
    "#     b = tf.constant([[1.0, 1.0], [0.0, 1.0]])\n",
    "    \n",
    "#     # Perform matrix multiplication\n",
    "#     c = tf.matmul(a, b)\n",
    "    \n",
    "#     # Print the result\n",
    "#     print(\"Result of matrix multiplication:\\n\", c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available. Here are the details:\n",
      " - GPU Name: NVIDIA TITAN RTX\n",
      " - Number of GPUs: 3\n",
      " - Current GPU: 0\n",
      "Performing a matrix multiplication to test the GPU...\n",
      "Matrix multiplication completed.\n",
      "Time taken: 0.025 seconds\n",
      "Device used for computation: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU is available. Here are the details:\")\n",
    "    print(\" - GPU Name:\", torch.cuda.get_device_name(0))\n",
    "    print(\" - Number of GPUs:\", torch.cuda.device_count())\n",
    "    print(\" - Current GPU:\", torch.cuda.current_device())\n",
    "else:\n",
    "    print(\"GPU is not available. Please check your CUDA installation.\")\n",
    "\n",
    "# Perform a simple computation to test the GPU\n",
    "a = torch.randn(1000, 1000, device='cuda')\n",
    "b = torch.randn(1000, 1000, device='cuda')\n",
    "\n",
    "print(\"Performing a matrix multiplication to test the GPU...\")\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "c = torch.matmul(a, b)\n",
    "\n",
    "print(\"Matrix multiplication completed.\")\n",
    "print(\"Time taken: {:.3f} seconds\".format(time.time() - start_time))\n",
    "print(\"Device used for computation:\", c.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8700\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.backends.cudnn.version())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wz_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
