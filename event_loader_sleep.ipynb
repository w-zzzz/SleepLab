{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries and define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATH = \"C:/Users/amd/OneDrive - National University of Singapore/SleepData\"\n",
    "# EDF_FILE_PATH = \"C:/Users/amd/OneDrive - National University of Singapore/SleepData/szu_hospital/PSG/2024-6-20jiangyifan.edf\"\n",
    "# PSG_FILE_PATH = \"C:/Users/amd/OneDrive - National University of Singapore/SleepData/szu_hospital/PSG/2024-6-20jiangyifan.edf\"\n",
    "# XML_FILE_PATH = \"psg/20240620江逸凡.edf.XML\"\n",
    "# SAVE_PATH = \"C:/Users/amd/OneDrive - National University of Singapore/SleepData/szu_hospital/PSG/merged_2024-6-20jiangyifan.edf.pkl\"\n",
    "# print(PSG_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/data/private/ZhouWenren/SleepLab/psg/2024-6-20jiangyifan.edf\n"
     ]
    }
   ],
   "source": [
    "# Load NeuroKit and other useful packages\n",
    "import neurokit2 as nk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.signal as signal\n",
    "from datetime import datetime, timedelta\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from cw_radar import *\n",
    "from psg import *\n",
    "import importlib\n",
    "import constants\n",
    "import os\n",
    "importlib.reload(constants)\n",
    "from constants import PSG_FILE_PATH, XML_FILE_PATH, SAVE_PATH, RADAR_FILE_PATH, FULL_SAVE_PATH\n",
    "\n",
    "def one_hot_encode(df_psg, df_xml, encode_type='stage'):\n",
    "    \"\"\"\n",
    "    One-hot encode events or sleep stages and integrate into the PSG DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - df_psg: DataFrame containing the PSG data.\n",
    "    - df_xml: XMLProcessor object containing event and sleep stage data.\n",
    "    - encode_type: str, either 'stage' or 'event' to specify what to encode.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: PSG DataFrame with one-hot encoded columns integrated.\n",
    "    \"\"\"\n",
    "    if encode_type == 'stage':\n",
    "        df_data = df_xml.sleep_stages\n",
    "        time_window = 30\n",
    "        unique_items = df_data['Sleep Stage'].unique()\n",
    "        time_column = 'Start Time'\n",
    "    elif encode_type == 'event':\n",
    "        df_data = df_xml.events\n",
    "        time_window = 1\n",
    "        unique_items = df_data['Name'].unique()\n",
    "        time_column = 'Start'\n",
    "    else:\n",
    "        raise ValueError(\"Invalid encode_type. Must be 'stage' or 'event'.\")\n",
    "\n",
    "    # Create an empty DataFrame with the same index as df_psg\n",
    "    one_hot_df = pd.DataFrame(index=df_psg.index)\n",
    "    time_window = timedelta(seconds=time_window)\n",
    "    \n",
    "    # Create one-hot encoded columns for each unique item\n",
    "    for item in unique_items:\n",
    "        one_hot_df[item] = 0\n",
    "\n",
    "    # Fill in the one-hot encoded values based on times\n",
    "    for _, row in df_data.iterrows():\n",
    "        start_time = row[time_column]\n",
    "        \n",
    "        if encode_type == 'stage':\n",
    "            end_time = start_time + time_window\n",
    "            item = row['Sleep Stage']\n",
    "            mask = (df_psg.index >= start_time) & (df_psg.index < end_time)\n",
    "\n",
    "        else:  # encode_type == 'event'\n",
    "            end_time = row['End']\n",
    "            item = row['Name']\n",
    "            mask = (df_psg.index >= start_time - time_window) & (df_psg.index < end_time + time_window) # label window: 1s before and after the event\n",
    "        \n",
    "        one_hot_df.loc[mask, item] = 1\n",
    "\n",
    "    return one_hot_df\n",
    "\n",
    "def integrate(df_psg, df_xml):\n",
    "    # One-hot encode events and sleep stages\n",
    "    events_one_hot = one_hot_encode(df_psg, df_xml, encode_type='event')\n",
    "    stages_one_hot = one_hot_encode(df_psg, df_xml, encode_type='stage')\n",
    "\n",
    "    # Integrate the one-hot encoded DataFrames into the PSG DataFrame\n",
    "    return pd.concat([df_psg, stages_one_hot, events_one_hot], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowpass_filter(fs, sig, highcut, order=4):\n",
    "    nyq = 0.5 * fs\n",
    "    high = highcut / nyq\n",
    "    b, a = signal.butter(order, high, btype='lowpass')\n",
    "    filtered_signal = signal.filtfilt(b, a, sig)\n",
    "    return filtered_signal\n",
    "\n",
    "def align_to_common_time_grid(psg_resampled, radar_resampled, freq_hz=64):\n",
    "    \"\"\"\n",
    "    Aligns two resampled dataframes to a common time grid.\n",
    "\n",
    "    Parameters:\n",
    "    psg_resampled (pd.DataFrame): The first resampled dataframe.\n",
    "    radar_resampled (pd.DataFrame): The second resampled dataframe.\n",
    "    freq_hz (int): The frequency for the common time grid in Hz. Default is 64Hz.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame, pd.DataFrame: The aligned dataframes.\n",
    "    \"\"\"\n",
    "    # Convert frequency from Hz to milliseconds\n",
    "    freq_ms = f'{1000 / freq_hz}ms'\n",
    "\n",
    "    # Define a common time grid (using the minimum and maximum timestamps)\n",
    "    common_time_index = pd.date_range(\n",
    "        start=max(psg_resampled.index.min(), radar_resampled.index.min()), \n",
    "        end=min(psg_resampled.index.max(), radar_resampled.index.max()), \n",
    "        freq=freq_ms  # Adjust frequency as necessary\n",
    "    )\n",
    "\n",
    "    # Reindex and interpolate both dataframes to this common time grid\n",
    "    psg_aligned = psg_resampled.reindex(common_time_index, method='nearest')\n",
    "    radar_aligned = radar_resampled.reindex(common_time_index, method='nearest')\n",
    "\n",
    "    return psg_aligned, radar_aligned\n",
    "\n",
    "def segment_and_label(df, segment_sec=10, freq_hz=64):\n",
    "    \"\"\"\n",
    "    Segments and labels the given DataFrame based on given time window.\n",
    "    Parameters:\n",
    "    - df (pandas.DataFrame): The DataFrame containing the data to be segmented and labeled.\n",
    "    - segment_sec (int): The duration of each segment in seconds. Default is 10.\n",
    "    - freq_hz (int): The frequency of the data in Hz. Default is 64.\n",
    "    Returns:\n",
    "    - segmented_df (pandas.DataFrame): The segmented and labeled DataFrame.\n",
    "    \"\"\"\n",
    "    # Initialize list to store segmented data\n",
    "    segmented_data = []\n",
    "    segment_size = segment_sec * freq_hz\n",
    "    \n",
    "    # Define columns for sleep stages and sleep events\n",
    "    sleep_stages = ['Movement Time (MT)', 'Wakefulness (W)', 'NREM Sleep Stage 1 (N1)', 'REM Sleep', \n",
    "                    'NREM Sleep Stage 2 (N2)', 'NREM Sleep Stage 3 (N3)']\n",
    "    sleep_events = ['Mixed Apnea', 'Limb Movement (Left)', 'Limb Movement (Right)', 'SpO2 desaturation', \n",
    "                    'Hypopnea', 'PLM (Left)', 'PLM (Right)', 'SpO2 artifact', 'Arousal (ARO SPONT)', \n",
    "                    'Arousal (ARO PLM)', 'Arousal (ARO Limb)', 'Arousal (ARO RES)', 'Central Apnea', \n",
    "                    'Obstructive Apnea']\n",
    "    \n",
    "    # Loop over the DataFrame in chunks of segment_size\n",
    "    for start in range(0, len(df), segment_size):\n",
    "        # Get current segment\n",
    "        segment = df.iloc[start:start + segment_size]\n",
    "        \n",
    "        # Calculate mean of continuous signals\n",
    "        continuous_data = segment[['processed_signal', 'ECG', 'Thor', 'Abdo', 'SpO2']].mean().to_dict()\n",
    "        \n",
    "        # Determine the dominant sleep stage (most frequent one)\n",
    "        sleep_stage_counts = segment[sleep_stages].sum()\n",
    "        dominant_sleep_stage = sleep_stage_counts.idxmax() if sleep_stage_counts.max() > 0 else 'No Stage'\n",
    "        \n",
    "        # Determine the dominant sleep event (most frequent one)\n",
    "        sleep_event_counts = segment[sleep_events].sum()\n",
    "        dominant_sleep_event = sleep_event_counts.idxmax() if sleep_event_counts.max() > 0 else 'No Event'\n",
    "        \n",
    "        # Get Start Time and End Time, trimmed to whole seconds\n",
    "        start_time = segment.index[0].floor('s')\n",
    "        end_time = segment.index[-1].floor('s')\n",
    "        \n",
    "        # Compile the data for this segment\n",
    "        segment_data = {'Start Time': start_time, 'End Time': end_time, **continuous_data, \n",
    "                        'Dominant Sleep Stage': dominant_sleep_stage, 'Dominant Sleep Event': dominant_sleep_event}\n",
    "        segmented_data.append(segment_data)\n",
    "    \n",
    "    # Convert segmented data to DataFrame\n",
    "    segmented_df = pd.DataFrame(segmented_data)\n",
    "    return segmented_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load PSG data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psg_file_path = PSG_FILE_PATH\n",
    "# psg_file_path = r\"C:\\Users\\amd\\OneDrive - National University of Singapore\\SleepData\\szu_hospital\\PSG\\2024-6-20jiangyifan.edf\"\n",
    "psg_processor = PSGDataProcessor(psg_file_path)\n",
    "psg_processor.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(psg_processor.data['ECG']))\n",
    "print(psg_processor.data['ECG'][0]) # ECG data\n",
    "print(len(psg_processor.data['ECG'][0][0]))\n",
    "print(psg_processor.data['ECG'][1]) # ECG timestamps\n",
    "print(len(psg_processor.data['ECG'][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract segments of ECG data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the start and stop times in seconds\n",
    "tmin, tmax = 10, 20  # Extract data between 10 and 20 seconds\n",
    "psg_time_segment = psg_processor.extract_data_by_range(tmin, tmax)\n",
    "print(psg_time_segment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data between two timestamps\n",
    "start_datetime = datetime(2024, 6, 20, 22, 10, 33) # Replace with your actual start datetime\n",
    "end_datetime = datetime(2024, 6, 20, 22, 11, 33)  # Replace with your actual end datetime\n",
    "data_types = ['ECG', 'Thor']  # Replace with your actual data types\n",
    "\n",
    "print(f\"Start Timestamp: {start_datetime}, End Timestamp: {end_datetime}\")  # Print the start and end timestamps of the extracted data\n",
    "psg_date_segment = psg_processor.extract_segment_by_timestamp(start_datetime, end_datetime, data_types)\n",
    "print(psg_date_segment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load XML data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xml_file_path = XML_FILE_PATH\n",
    "start_datetime_str = \"2024-06-20 22:02:34\"\n",
    "xml_processor = XMLProcessor(xml_file_path, start_datetime_str)\n",
    "\n",
    "xml_processor.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xml_processor.sleep_stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xml_processor.events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unique events and sleep stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_event_names = xml_processor.events['Name'].unique()\n",
    "print(unique_event_names)\n",
    "\n",
    "unique_sleep_stages = xml_processor.sleep_stages['Sleep Stage'].unique()\n",
    "print(unique_sleep_stages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter by event type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming xml_processor.df_events is your DataFrame\n",
    "xml_filtered_by_type = xml_processor.events[xml_processor.events['Name'].str.contains('limb', case=False)]\n",
    "xml_filtered_by_type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Events filtered by time range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows where 'timestamp' is within the specified range\n",
    "start_datetime = datetime(2024, 6, 20, 22, 10, 33) # Replace with your actual start datetime\n",
    "end_datetime = datetime(2024, 6, 20, 22, 11, 33)  # Replace with your actual end datetime\n",
    "xml_filtered_by_type = xml_filtered_by_type[(xml_filtered_by_type['Start'] >= start_datetime) & (xml_filtered_by_type['Start'] <= end_datetime)]\n",
    "xml_filtered_by_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(psg_date_segment)\n",
    "print(xml_filtered_by_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Events sorted by duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "durations = xml_processor.events['Duration']\n",
    "print(durations.describe())\n",
    "xml_sorted_by_duration = xml_processor.events.sort_values(by='Duration', ascending=False)\n",
    "print(xml_sorted_by_duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge the two dataframes: PSG(bio signals) and XML(event labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load PSG and XML data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# psg_file_path = r\"C:\\Users\\amd\\OneDrive - National University of Singapore\\SleepData\\szu_hospital\\PSG\\2024-6-20jiangyifan.edf\"\n",
    "psg_file_path = PSG_FILE_PATH\n",
    "psg_processor = PSGDataProcessor(psg_file_path)\n",
    "psg_processor.load_data()\n",
    "\n",
    "xml_file_path = XML_FILE_PATH\n",
    "start_datetime_str = \"2024-06-20 22:02:34\"\n",
    "xml_processor = XMLProcessor(xml_file_path, start_datetime_str)\n",
    "xml_processor.load()\n",
    "\n",
    "df_sleep_events = xml_processor.events\n",
    "print(psg_processor.start_datetime)\n",
    "print(xml_processor.start_datetime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract the time range of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Start Timestamp: {psg_processor.start_datetime}, End Timestamp: {psg_processor.end_datetime}\")\n",
    "\n",
    "# Start Timestamp: 2024-06-20 22:02:34, End Timestamp: 2024-06-21 05:54:58.999023\n",
    "start_datetime = datetime(2024, 6, 20, 22, 2, 34) # Replace with your actual start datetime\n",
    "end_datetime = datetime(2024, 6, 21, 5, 54, 58)  # Replace with your actual end datetime\n",
    "# # Extract data between two timestamps\n",
    "# start_datetime = datetime(2024, 6, 20, 22, 10, 33) # Replace with your actual start datetime\n",
    "# end_datetime = datetime(2024, 6, 20, 22, 11, 33)  # Replace with your actual end datetime\n",
    "data_types = ['ECG', 'Thor']  # Replace with your actual data types\n",
    "\n",
    "print(f\"Start Timestamp: {start_datetime}, End Timestamp: {end_datetime}\")  # Print the start and end timestamps of the extracted data\n",
    "psg_date_segment = psg_processor.extract_segment_by_timestamp(start_datetime, end_datetime, data_types)\n",
    "print(psg_date_segment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the start and stop times in seconds\n",
    "tmin, tmax = 0, 60  # Extract data between 10 and 20 seconds\n",
    "data_types = ['ECG', 'Thor']  # Replace with your actual data types\n",
    "\n",
    "start_dt, end_dt, psg_time_segment = psg_processor.extract_data_by_range(tmin, tmax)\n",
    "psg_time_segment = psg_time_segment[data_types]\n",
    "print(f\"Start time: {start_dt}, End time: {end_dt}\")\n",
    "print(psg_time_segment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df_psg is your processed PSG DataFrame\n",
    "df_psg = psg_date_segment\n",
    "# df_psg = psg_time_segment\n",
    "\n",
    "# One-hot encode events and sleep stages\n",
    "# events_one_hot_e = one_hot_encode(df_psg, xml_processor, 'event')\n",
    "# events_one_hot_s = one_hot_encode(df_psg, xml_processor, 'stage')\n",
    "\n",
    "merged_df = integrate(df_psg, xml_processor)\n",
    "print(merged_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine the correctness of the one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = xml_processor.sleep_stages\n",
    "s_filtered = s[(s['Start Time'] >= start_datetime) & (s['Start Time'] <= end_datetime)]\n",
    "\n",
    "# Filter rows within the time range\n",
    "t_filtered = merged_df.loc[start_datetime:end_datetime]\n",
    "\n",
    "# Extract the specified columns from filtered_temp\n",
    "wake1 = t_filtered[t_filtered['Wakefulness (W)'] == 1]\n",
    "print(wake1.head().index[0])\n",
    "wake2 = s_filtered[s_filtered['Sleep Stage'] == 'Wakefulness (W)']\n",
    "print(wake2)\n",
    "\n",
    "stage1 = t_filtered[t_filtered['NREM Sleep Stage 1 (N1)'] == 1]\n",
    "print(stage1.head().index[0])\n",
    "stage2 = s_filtered[s_filtered['Sleep Stage'] == 'NREM Sleep Stage 1 (N1)']\n",
    "print(stage2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the complete(merged) DataFrame to a CSV/Pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file path and get the directory path and filename\n",
    "# psg_file_path = \"/Users/w.z/Library/CloudStorage/OneDrive-NationalUniversityofSingapore/SleepData/苏州大学附属医院/PSG/2024-6-20jiangyifan.edf\"\n",
    "psg_file_path = PSG_FILE_PATH\n",
    "dir_path, filename = os.path.split(psg_file_path)\n",
    "modified_filename = 'merged_' + filename\n",
    "\n",
    "# # Save as CSV file for portability\n",
    "# save_path = os.path.join(dir_path, modified_filename + '.csv')\n",
    "# merged_df.to_csv(save_path, index=False)  # Set index=False if you don't want to save the index\n",
    "# print(f\"CSV file saved at: {save_path}\")\n",
    "\n",
    "# Save as Pickle file for efficiency and compactness\n",
    "save_path = os.path.join(dir_path, modified_filename + '.pkl')\n",
    "merged_df.to_pickle(save_path)\n",
    "print(f\"Pickle file saved at: {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the merged DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to the pickle file\n",
    "save_path = SAVE_PATH\n",
    "pickle_path = save_path # '/path/to/your/filename.pkl'\n",
    "psg_date_segment = pd.read_pickle(pickle_path)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(psg_date_segment.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ERA(Event-related Analysis): Sleep Stages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sleep_stages = xml_processor.sleep_stages\n",
    "df_sleep_stages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes = xml_processor.sleep_stages['Sleep Stage Code'].values\n",
    "print(np.unique(codes))\n",
    "\n",
    "stages = xml_processor.sleep_stages['Sleep Stage'].values\n",
    "print(np.unique(stages))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare the data (Time window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(psg_processor.start_datetime)\n",
    "print(psg_processor.end_datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_datetime = datetime(2024, 6, 20, 22, 40, 33) # Replace with your actual start datetime\n",
    "# end_datetime = datetime(2024, 6, 20, 23, 10, 33)  # Replace with your actual end datetime\n",
    "start_datetime = datetime(2024, 6, 20, 22, 40, 33) # Replace with your actual start datetime\n",
    "end_datetime = datetime(2024, 6, 20, 23, 00, 33)  # Replace with your actual end datetime\n",
    "\n",
    "start_time = start_datetime\n",
    "end_time = end_datetime\n",
    "\n",
    "filtered_df = psg_date_segment.loc[start_time:end_time]\n",
    "df_subset = filtered_df[['ECG', 'Thor']]\n",
    "print(df_subset.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare the data (add columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_sleep_stage_column(df_subset, df_sleep_stages):\n",
    "    # Create a new column in df_subset to store the sleep stage code\n",
    "    df_subset['Stage Code'] = None\n",
    "\n",
    "    # Iterate over each row in df_sleep_stages\n",
    "    for index, row in df_sleep_stages.iterrows():\n",
    "        stage_start_time = row['Start Time']\n",
    "        stage_end_time = stage_start_time + timedelta(seconds=30)\n",
    "\n",
    "        # Find the rows in df_subset that fall within the current sleep stage interval\n",
    "        mask = (df_subset.index >= stage_start_time) & (df_subset.index < stage_end_time)\n",
    "        \n",
    "        # Assign the sleep stage code to the corresponding rows in df_subset\n",
    "        df_subset.loc[mask, 'Stage Code'] = row['Sleep Stage Code']\n",
    "        # df_subset.loc[mask, 'Sleep Stage'] = row['Sleep Stage']\n",
    "        # df_subset.loc[mask, 'Time (seconds)'] = row['Time (seconds)']\n",
    "        # df_subset.loc[mask, 'Duration'] = row['Duration']\n",
    "    \n",
    "    return df_subset\n",
    "\n",
    "# Example usage\n",
    "# Apply the function to add the sleep stage column\n",
    "df_subset_with_sleep_stage = add_sleep_stage_column(df_subset, df_sleep_stages)\n",
    "\n",
    "print(df_subset_with_sleep_stage)\n",
    "print(df_subset_with_sleep_stage['Stage Code'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# psg_processor.signals_diagram(df_subset_with_sleep_stage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Events (Auto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process and plot the events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define events based on sleep stages\n",
    "# df_subset_with_sleep_stage['ECG'] = df_subset_with_sleep_stage['ECG'] *100000\n",
    "# df_subset_with_sleep_stage['Thor'] = df_subset_with_sleep_stage['Thor'] *100000\n",
    "events1 = nk.events_find(df_subset_with_sleep_stage['Stage Code'], threshold=0, threshold_keep='below')\n",
    "print(events1)\n",
    "plot = nk.events_plot(events1, df_subset_with_sleep_stage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events2 = nk.events_find(df_subset_with_sleep_stage['Stage Code'], threshold=0, threshold_keep='above')\n",
    "print(events2)\n",
    "plot = nk.events_plot(events2, df_subset_with_sleep_stage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(events1)\n",
    "print(events2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Combine the arrays\n",
    "combined_onset = np.concatenate((events1['onset'], events2['onset']))\n",
    "combined_duration = np.concatenate((events1['duration'], events2['duration']))\n",
    "\n",
    "# Sort the arrays based on the combined onset values\n",
    "sorted_indices = np.argsort(combined_onset)\n",
    "sorted_onset = combined_onset[sorted_indices]\n",
    "sorted_duration = combined_duration[sorted_indices]\n",
    "sorted_label = [str(i+1) for i in range(len(sorted_onset))]\n",
    "\n",
    "\n",
    "# Create the merged dictionary\n",
    "merged_events = {\n",
    "    'onset': sorted_onset,\n",
    "    'duration': sorted_duration,\n",
    "    'label': sorted_label\n",
    "}\n",
    "\n",
    "print(merged_events)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = nk.events_plot(merged_events, df_subset_with_sleep_stage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot RSP diagram (trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psg_processor.start_datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_types = ['Thor']\n",
    "start_datetime = datetime(2024, 6, 20, 22, 58, 33) # Replace with your actual start datetime\n",
    "end_datetime = datetime(2024, 6, 20, 22, 59, 33)  # Replace with your actual end datetime\n",
    "extracted_data = psg_processor.extract_segment_by_timestamp(start_datetime, end_datetime, extracted_types)\n",
    "rsp_signals, rsp_info = psg_processor.rsp_diagram(extracted_data['Thor'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process the Signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the signal\n",
    "data_clean, info = nk.bio_process(ecg=df_subset_with_sleep_stage[\"ECG\"], \n",
    "                                  rsp=df_subset_with_sleep_stage[\"Thor\"],\n",
    "                                  keep=df_subset_with_sleep_stage[\"Stage Code\"],  \n",
    "                                  sampling_rate=1024)\n",
    "\n",
    "# Visualize some of the channels\n",
    "data_clean[[\"ECG_Rate\", \"RSP_Rate\", \"Stage Code\"]].plot(subplots=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df_subset_with_sleep_stage['Stage Code'].values\n",
    "\n",
    "plt.figure(figsize=(6, 3))  # Set the figure size for better readability\n",
    "plt.plot(x, marker='o', linestyle='-', color='b')  # Plot x with markers and lines\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and plot epochs\n",
    "epochs = nk.epochs_create(data_clean, merged_events, sampling_rate=1024, epochs_start=-30, epochs_end=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch_index, epoch in epochs.items():\n",
    "    for attribute, value in epoch.items():\n",
    "        print(f\"  {attribute}\")\n",
    "    print(\"\\n\")  # Adds a newline for better readability between epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through epoch data\n",
    "for epoch in epochs.values():\n",
    "    # Plot scaled signals, \"Stage Code\"\n",
    "    nk.signal_plot(epoch[['ECG_Rate', 'RSP_Rate']], \n",
    "                   title=epoch['Label'].values[0],  # Extract condition name\n",
    "                   standardize=True)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interval Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = nk.events_plot(merged_events,data_clean[[\"ECG_Rate\", \"RSP_Rate\", \"Stage Code\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(merged_events)\n",
    "epochs = nk.epochs_create(data_clean, merged_events, sampling_rate=1024, epochs_start=0, epochs_end='from_events')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze\n",
    "nk.ecg_intervalrelated(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nk.rsp_intervalrelated(epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot ECG and RSP signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30, 6))  # Set figure size for better readability\n",
    "plt.plot(df_subset['ECG'], label='ECG')  # Plot the ECG column\n",
    "plt.title('ECG Over Time')  # Title of the plot\n",
    "plt.xlabel('Index')  # X-axis label\n",
    "plt.ylabel('ECG Value')  # Y-axis label\n",
    "plt.legend()  # Show legend\n",
    "plt.show()  # Display the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30, 6))  # Set figure size for better readability\n",
    "plt.plot(df_subset['Thor'], label='RSP')  # Plot the ECG column\n",
    "plt.title('RSP Over Time')  # Title of the plot\n",
    "plt.xlabel('Index')  # X-axis label\n",
    "plt.ylabel('RPS Value')  # Y-axis label\n",
    "plt.legend()  # Show legend\n",
    "plt.show()  # Display the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Extraction (Auto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = nk.bio_analyze(epochs, sampling_rate=1024)\n",
    "# df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Extraction (Manual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = {}  # Initialize an empty dict to store the results\n",
    "         \n",
    "# Iterate through epochs index and data\n",
    "for epoch_index, epoch in epochs.items():\n",
    "    df_temp[epoch_index] = {}  # Initialize an empty dict inside of it\n",
    "                            \n",
    "\n",
    "    # Note: We will use the 100th value (corresponding to the event onset, 0s) as the baseline\n",
    "\n",
    "    # ECG ====\n",
    "    ecg_baseline = epoch[\"ECG_Rate\"].values[100]  # Baseline\n",
    "    ecg_mean = epoch[\"ECG_Rate\"][0:4].mean()  # Mean heart rate in the 0-4 seconds\n",
    "    # Store ECG in df_temp\n",
    "    df_temp[epoch_index][\"ECG_Rate_Mean\"] = ecg_mean - ecg_baseline  # Correct for baseline\n",
    "\n",
    "    # RSP ====\n",
    "    rsp_baseline = epoch[\"RSP_Rate\"].values[100]  # Baseline\n",
    "    rsp_rate = epoch[\"RSP_Rate\"][0:6].mean()  # Longer window for RSP that has a slower dynamic\n",
    "    # Store RSP in df_temp\n",
    "    df_temp[epoch_index][\"RSP_Rate_Mean\"] = rsp_rate - rsp_baseline  # Correct for baseline\n",
    "\n",
    "df_temp = pd.DataFrame.from_dict(df_temp, orient=\"index\")  # Convert to a dataframe\n",
    "df_temp  # Print DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Event-Related Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reset = df_temp.reset_index()\n",
    "print(df_reset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.boxplot(x=\"index\", y=\"ECG_Rate_Mean\", data=df_reset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.boxplot(x=\"index\", y=\"RSP_Rate_Mean\", data=df_reset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Events (Manual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sleep_stages['Duration'] = 30\n",
    "df_sleep_stages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare the data (add columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_sleep_stage_column(df_subset, df_sleep_stages):\n",
    "    # Create a new column in df_subset to store the sleep stage code\n",
    "    df_subset['Stage Code'] = None\n",
    "\n",
    "    # Iterate over each row in df_sleep_stages\n",
    "    for index, row in df_sleep_stages.iterrows():\n",
    "        stage_start_time = row['Start Time']\n",
    "        stage_end_time = stage_start_time + timedelta(seconds=30)\n",
    "\n",
    "        # Find the rows in df_subset that fall within the current sleep stage interval\n",
    "        mask = (df_subset.index >= stage_start_time) & (df_subset.index < stage_end_time)\n",
    "        \n",
    "        # Assign the sleep stage code to the corresponding rows in df_subset\n",
    "        df_subset.loc[mask, 'Stage Code'] = row['Sleep Stage Code']\n",
    "        df_subset.loc[mask, 'Sleep Stage'] = row['Sleep Stage']\n",
    "        df_subset.loc[mask, 'Time (seconds)'] = row['Time (seconds)']\n",
    "        df_subset.loc[mask, 'Duration'] = row['Duration']\n",
    "    \n",
    "    return df_subset\n",
    "\n",
    "# Example usage\n",
    "# Apply the function to add the sleep stage column\n",
    "df_subset_sleep = add_sleep_stage_column(df_subset, df_sleep_stages)\n",
    "\n",
    "print(df_subset_sleep)\n",
    "print(df_subset_sleep['Stage Code'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sleep_stages(df_sleep_stages, start_datetime, end_datetime):\n",
    "    \n",
    "    # Calculate 'End Time' assuming each stage lasts 30 seconds\n",
    "    df_sleep_stages['End Time'] = df_sleep_stages['Start Time'] + pd.Timedelta(seconds=30)\n",
    "    df_subset = df_sleep_stages[(df_sleep_stages['Start Time'] >= start_datetime) & (df_sleep_stages['End Time'] <= end_datetime)]\n",
    "    \n",
    "    return df_subset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_datetime = datetime(2024, 6, 20, 22, 40, 33) # Replace with your actual start datetime\n",
    "end_datetime = datetime(2024, 6, 20, 23, 10, 33)  # Replace with your actual end datetime\n",
    "\n",
    "df_subset_sleep = extract_sleep_stages(df_sleep_stages, start_datetime, end_datetime)\n",
    "df_subset_sleep.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sleep_stages_dur = df_subset_sleep\n",
    "event_onsets = df_sleep_stages_dur['Time (seconds)'].values\n",
    "event_durations = df_sleep_stages_dur['Duration'].values\n",
    "event_conditions = df_sleep_stages_dur['Sleep Stage'].values\n",
    "\n",
    "events = nk.events_create(event_onsets = event_onsets,\n",
    "                          event_durations = event_durations,\n",
    "                          event_conditions= event_conditions)\n",
    "\n",
    "print(events)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_subset)\n",
    "print(df_subset_sleep['Sleep Stage Code'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot the location of event with the signals\n",
    "plot = nk.events_plot(events, df_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IRA(Interval-related Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the signal\n",
    "data_clean, info = nk.bio_process(ecg=df_subset_with_sleep_stage[\"ECG\"], \n",
    "                                  rsp=df_subset_with_sleep_stage[\"Thor\"],\n",
    "                                  keep=df_subset_with_sleep_stage[\"Stage Code\"],  \n",
    "                                  sampling_rate=1024)\n",
    "\n",
    "# Visualize some of the channels\n",
    "data_clean[[\"ECG_Rate\", \"RSP_Rate\", \"Stage Code\"]].plot(subplots=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, info = nk.ecg_process(df_subset_with_sleep_stage[\"ECG\"], sampling_rate=1024)\n",
    "nk.ecg_intervalrelated(data, sampling_rate=1024)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs = nk.epochs_create(df, events=[0, 15000], sampling_rate=100, epochs_end=150)\n",
    "\n",
    "# # Half the data\n",
    "# epochs = nk.epochs_create(ecg_signals, \n",
    "#                           events=[0, 15000], \n",
    "#                           sampling_rate=100, \n",
    "#                           epochs_start=0, \n",
    "#                           epochs_end=150)\n",
    "\n",
    "# nk.ecg_intervalrelated(epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ERA(Event-related Analysis): Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check which columns of the df_suset have non-zero values within the given time range and return the column names\n",
    "def check_non_zero_columns(df, start_datetime, end_datetime):\n",
    "    # Filter rows within the time range\n",
    "    df = df.loc[start_datetime:end_datetime]\n",
    "    \n",
    "    # Define the list of columns to extract\n",
    "    columns = df.columns\n",
    "    \n",
    "    # Check which columns have non-zero values\n",
    "    non_zero_columns = df.columns[(df != 0).any()]\n",
    "    \n",
    "    return non_zero_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = xml_processor.events['Name'].values\n",
    "print(np.unique(events))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limb Movement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_type = 'Limb Movement (Left)'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare the data (Time window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_datetime = datetime(2024, 6, 20, 22, 40, 33) # Replace with your actual start datetime\n",
    "# end_datetime = datetime(2024, 6, 20, 23, 10, 33)  # Replace with your actual end datetime\n",
    "start_datetime = datetime(2024, 6, 20, 22, 8, 0) # 2024-06-20 22:08:00\n",
    "end_datetime = datetime(2024, 6, 20, 22, 11, 0) # 2024-06-20 22:11:00\n",
    "\n",
    "start_time = start_datetime\n",
    "end_time = end_datetime\n",
    "\n",
    "filtered_df = psg_date_segment.loc[start_time:end_time]\n",
    "# Make a df subset with all the columns except: \n",
    "# 'ECG', 'Thor', 'Movement Time (MT)', 'Wakefulness (W)', 'NREM Sleep Stage 1 (N1)', \n",
    "# 'REM Sleep', 'NREM Sleep Stage 2 (N2)', 'NREM Sleep Stage 3 (N3)'\n",
    "columns_to_drop = ['Movement Time (MT)', 'Wakefulness (W)', 'NREM Sleep Stage 1 (N1)', 'REM Sleep', 'NREM Sleep Stage 2 (N2)', 'NREM Sleep Stage 3 (N3)']\n",
    "df_subset = filtered_df.drop(columns=columns_to_drop)\n",
    "print(df_subset.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find Events (Auto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process and plot the events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage: \n",
    "non_zero_columns = check_non_zero_columns(psg_date_segment, start_datetime, end_datetime)\n",
    "print(non_zero_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define events for specific event types (label value above threshold: 0)\n",
    "# , df_subset['Limb Movement (Right)']\n",
    "events3 = nk.events_find(df_subset[event_type], threshold=0, threshold_keep='above')\n",
    "print(events3)\n",
    "plot = nk.events_plot(events3, df_subset[['ECG', 'Thor']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events4 = nk.events_find(df_subset[event_type], threshold=0, threshold_keep='below')\n",
    "print(events4)\n",
    "plot = nk.events_plot(events4, df_subset[['ECG', 'Thor']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the arrays\n",
    "combined_onset = np.concatenate((events3['onset'], events4['onset']))\n",
    "combined_duration = np.concatenate((events3['duration'], events4['duration']))\n",
    "\n",
    "# Sort the arrays based on the combined onset values\n",
    "sorted_indices = np.argsort(combined_onset)\n",
    "sorted_onset = combined_onset[sorted_indices]\n",
    "sorted_duration = combined_duration[sorted_indices]\n",
    "sorted_label = [str(i+1) for i in range(len(sorted_onset))]\n",
    "\n",
    "\n",
    "# Create the merged dictionary\n",
    "merged_events = {\n",
    "    'onset': sorted_onset,\n",
    "    'duration': sorted_duration,\n",
    "    'label': sorted_label\n",
    "}\n",
    "\n",
    "print(merged_events)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = nk.events_plot(merged_events, df_subset[['ECG', 'Thor']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process the Signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the signal\n",
    "data_clean, info = nk.bio_process(ecg=df_subset[\"ECG\"], \n",
    "                                  rsp=df_subset[\"Thor\"],\n",
    "                                  keep=df_subset[event_type],  \n",
    "                                  sampling_rate=1024)\n",
    "\n",
    "# Visualize some of the channels\n",
    "data_clean[[\"ECG_Rate\", \"RSP_Rate\", event_type]].plot(subplots=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and plot epochs\n",
    "epochs = nk.epochs_create(data_clean, merged_events, sampling_rate=1024, epochs_start=-30, epochs_end=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch_index, epoch in epochs.items():\n",
    "    for attribute, value in epoch.items():\n",
    "        print(f\"  {attribute}\")\n",
    "    print(\"\\n\")  # Adds a newline for better readability between epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through epoch data\n",
    "for epoch in epochs.values():\n",
    "    # Plot scaled signals, \"Stage Code\"\n",
    "    nk.signal_plot(epoch[['ECG_Rate', 'RSP_Rate']], \n",
    "                   title=epoch['Label'].values[0],  # Extract condition name\n",
    "                   standardize=True)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interval Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = nk.events_plot(merged_events,data_clean[[\"ECG_Rate\", \"RSP_Rate\", event_type]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(merged_events)\n",
    "epochs = nk.epochs_create(data_clean, merged_events, sampling_rate=1024, epochs_start=0, epochs_end='from_events')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs['1'] # less than 10s (9.65s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs['3'] # greater than 10s (158s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs['4'] # less than 10s (4.3s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze\n",
    "nk.ecg_intervalrelated(epochs['3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nk.rsp_intervalrelated(epochs['3'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypopnea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_type = 'Hypopnea'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pnea_df = df_sleep_events[df_sleep_events['Name'].str.contains('pnea')]\n",
    "pnea_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare the data (Time window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2024-06-20 22:14:58.200\t2024-06-20 22:15:10.130\n",
    "start_datetime = datetime(2024, 6, 20, 22, 13, 00)\n",
    "end_datetime = datetime(2024, 6, 20, 22, 16, 00) \n",
    "\n",
    "start_time = start_datetime\n",
    "end_time = end_datetime\n",
    "\n",
    "filtered_df = psg_date_segment.loc[start_time:end_time]\n",
    "# Make a df subset with all the columns except: \n",
    "# 'ECG', 'Thor', 'Movement Time (MT)', 'Wakefulness (W)', 'NREM Sleep Stage 1 (N1)', \n",
    "# 'REM Sleep', 'NREM Sleep Stage 2 (N2)', 'NREM Sleep Stage 3 (N3)'\n",
    "columns_to_drop = ['Movement Time (MT)', 'Wakefulness (W)', 'NREM Sleep Stage 1 (N1)', 'REM Sleep', 'NREM Sleep Stage 2 (N2)', 'NREM Sleep Stage 3 (N3)']\n",
    "df_subset = filtered_df.drop(columns=columns_to_drop)\n",
    "print(df_subset.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare the data (add columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find Events (Auto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process and plot the events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage: \n",
    "non_zero_columns = check_non_zero_columns(psg_date_segment, start_datetime, end_datetime)\n",
    "print(non_zero_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define events for specific event types (label value above threshold: 0)\n",
    "# , df_subset['Limb Movement (Right)']\n",
    "events3 = nk.events_find(df_subset[event_type], threshold=0, threshold_keep='above')\n",
    "print(events3)\n",
    "plot = nk.events_plot(events3, df_subset[['ECG', 'Thor']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events4 = nk.events_find(df_subset[event_type], threshold=0, threshold_keep='below')\n",
    "print(events4)\n",
    "plot = nk.events_plot(events4, df_subset[['ECG', 'Thor']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the arrays\n",
    "combined_onset = np.concatenate((events3['onset'], events4['onset']))\n",
    "combined_duration = np.concatenate((events3['duration'], events4['duration']))\n",
    "\n",
    "# Sort the arrays based on the combined onset values\n",
    "sorted_indices = np.argsort(combined_onset)\n",
    "sorted_onset = combined_onset[sorted_indices]\n",
    "sorted_duration = combined_duration[sorted_indices]\n",
    "sorted_label = [str(i+1) for i in range(len(sorted_onset))]\n",
    "\n",
    "\n",
    "# Create the merged dictionary\n",
    "merged_events = {\n",
    "    'onset': sorted_onset,\n",
    "    'duration': sorted_duration,\n",
    "    'label': sorted_label\n",
    "}\n",
    "\n",
    "print(merged_events)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = nk.events_plot(merged_events, df_subset[['ECG', 'Thor']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process the Signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset[\"ECG\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(30, 6))  # Set figure size for better readability\n",
    "plt.plot(df_subset['ECG'], label='ECG')  # Plot the ECG column\n",
    "plt.title('ECG Over Time')  # Title of the plot\n",
    "plt.xlabel('Index')  # X-axis label\n",
    "plt.ylabel('ECG Value')  # Y-axis label\n",
    "plt.legend()  # Show legend\n",
    "plt.show()  # Display the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30, 6))  # Set figure size for better readability\n",
    "plt.plot(df_subset['Thor'], label='RSP')  # Plot the ECG column\n",
    "plt.title('RSP Over Time')  # Title of the plot\n",
    "plt.xlabel('Index')  # X-axis label\n",
    "plt.ylabel('RPS Value')  # Y-axis label\n",
    "plt.legend()  # Show legend\n",
    "plt.show()  # Display the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the signal\n",
    "data_clean, info = nk.bio_process(\n",
    "                                  ecg=df_subset[\"ECG\"],\n",
    "                                  rsp=df_subset[\"Thor\"],\n",
    "                                  keep=df_subset[event_type],  \n",
    "                                  sampling_rate=1024)\n",
    "\n",
    "# Visualize some of the channels\n",
    "data_clean[[\"ECG_Rate\", \"RSP_Rate\", event_type]].plot(subplots=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and plot epochs\n",
    "epochs = nk.epochs_create(data_clean, merged_events, sampling_rate=1024, epochs_start=-30, epochs_end=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch_index, epoch in epochs.items():\n",
    "    for attribute, value in epoch.items():\n",
    "        print(f\"  {attribute}\")\n",
    "    print(\"\\n\")  # Adds a newline for better readability between epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through epoch data\n",
    "for epoch in epochs.values():\n",
    "    # Plot scaled signals, \"Stage Code\"\n",
    "    nk.signal_plot(epoch[['ECG_Rate', 'RSP_Rate']], \n",
    "                   title=epoch['Label'].values[0],  # Extract condition name\n",
    "                   standardize=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interval Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = nk.events_plot(merged_events,data_clean[[\"ECG_Rate\", \"RSP_Rate\", event_type]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(merged_events)\n",
    "epochs = nk.epochs_create(data_clean, merged_events, sampling_rate=1024, epochs_start=0, epochs_end='from_events')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nk.rsp_intervalrelated(epochs['1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nk.rsp_intervalrelated(epochs['2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nk.rsp_intervalrelated(epochs['4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nk.rsp_intervalrelated(epochs['5'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Central Apnea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_type = 'Central Apnea'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pnea_df = df_sleep_events[df_sleep_events['Name'].str.contains(event_type)]\n",
    "pnea_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare the data (Time window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2024-06-21 03:33:36.600\t2024-06-21 03:33:54.830\n",
    "start_datetime = datetime(2024, 6, 21, 3, 33, 00)\n",
    "end_datetime = datetime(2024, 6, 21, 3, 35, 00) \n",
    "\n",
    "start_time = start_datetime\n",
    "end_time = end_datetime\n",
    "\n",
    "filtered_df = psg_date_segment.loc[start_time:end_time]\n",
    "# Make a df subset with all the columns except: \n",
    "# 'ECG', 'Thor', 'Movement Time (MT)', 'Wakefulness (W)', 'NREM Sleep Stage 1 (N1)', \n",
    "# 'REM Sleep', 'NREM Sleep Stage 2 (N2)', 'NREM Sleep Stage 3 (N3)'\n",
    "columns_to_drop = ['Movement Time (MT)', 'Wakefulness (W)', 'NREM Sleep Stage 1 (N1)', 'REM Sleep', 'NREM Sleep Stage 2 (N2)', 'NREM Sleep Stage 3 (N3)']\n",
    "df_subset = filtered_df.drop(columns=columns_to_drop)\n",
    "print(df_subset.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find Events (Auto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process and plot the events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage: \n",
    "non_zero_columns = check_non_zero_columns(psg_date_segment, start_datetime, end_datetime)\n",
    "print(non_zero_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define events for specific event types (label value above threshold: 0)\n",
    "# , df_subset['Limb Movement (Right)']\n",
    "events3 = nk.events_find(df_subset[event_type], threshold=0, threshold_keep='above')\n",
    "print(events3)\n",
    "plot = nk.events_plot(events3, df_subset[['ECG', 'Thor']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events4 = nk.events_find(df_subset[event_type], threshold=0, threshold_keep='below')\n",
    "print(events4)\n",
    "plot = nk.events_plot(events4, df_subset[['ECG', 'Thor']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the arrays\n",
    "combined_onset = np.concatenate((events3['onset'], events4['onset']))\n",
    "combined_duration = np.concatenate((events3['duration'], events4['duration']))\n",
    "\n",
    "# Sort the arrays based on the combined onset values\n",
    "sorted_indices = np.argsort(combined_onset)\n",
    "sorted_onset = combined_onset[sorted_indices]\n",
    "sorted_duration = combined_duration[sorted_indices]\n",
    "sorted_label = [str(i+1) for i in range(len(sorted_onset))]\n",
    "\n",
    "\n",
    "# Create the merged dictionary\n",
    "merged_events = {\n",
    "    'onset': sorted_onset,\n",
    "    'duration': sorted_duration,\n",
    "    'label': sorted_label\n",
    "}\n",
    "\n",
    "print(merged_events)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = nk.events_plot(merged_events, df_subset[['ECG', 'Thor']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process the Signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset[\"ECG\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(30, 6))  # Set figure size for better readability\n",
    "plt.plot(df_subset['ECG'], label='ECG')  # Plot the ECG column\n",
    "plt.title('ECG Over Time')  # Title of the plot\n",
    "plt.xlabel('Index')  # X-axis label\n",
    "plt.ylabel('ECG Value')  # Y-axis label\n",
    "plt.legend()  # Show legend\n",
    "plt.show()  # Display the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30, 6))  # Set figure size for better readability\n",
    "plt.plot(df_subset['Thor'], label='RSP')  # Plot the ECG column\n",
    "plt.title('RSP Over Time')  # Title of the plot\n",
    "plt.xlabel('Index')  # X-axis label\n",
    "plt.ylabel('RPS Value')  # Y-axis label\n",
    "plt.legend()  # Show legend\n",
    "plt.show()  # Display the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the signal\n",
    "data_clean, info_rsp = nk.bio_process(\n",
    "                                  ecg=df_subset[\"ECG\"],\n",
    "                                  rsp=df_subset[\"Thor\"],\n",
    "                                  keep=df_subset[event_type],  \n",
    "                                  sampling_rate=1024)\n",
    "\n",
    "# Visualize some of the channels\n",
    "data_clean[[\"ECG_Rate\", \"RSP_Rate\", event_type]].plot(subplots=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and plot epochs\n",
    "epochs = nk.epochs_create(data_clean, merged_events, sampling_rate=1024, epochs_start=-30, epochs_end=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch_index, epoch in epochs.items():\n",
    "    for attribute, value in epoch.items():\n",
    "        print(f\"  {attribute}\")\n",
    "    print(\"\\n\")  # Adds a newline for better readability between epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through epoch data\n",
    "for epoch in epochs.values():\n",
    "    # Plot scaled signals, \"Stage Code\"\n",
    "    nk.signal_plot(epoch[['ECG_Rate', 'RSP_Rate']], \n",
    "                   title=epoch['Label'].values[0],  # Extract condition name\n",
    "                   standardize=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interval Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = nk.events_plot(merged_events,data_clean[[\"ECG_Rate\", \"RSP_Rate\", event_type]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(merged_events)\n",
    "epochs = nk.epochs_create(data_clean, merged_events, sampling_rate=1024, epochs_start=0, epochs_end='from_events')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nk.ecg_intervalrelated(epochs[\"1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nk.rsp_intervalrelated(epochs[\"1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pnea_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2024-06-21 03:33:36.600\t2024-06-21 03:33:54.830\n",
    "start_datetime = datetime(2024, 6, 21, 3, 32, 00)\n",
    "end_datetime = datetime(2024, 6, 21, 3, 34, 00) \n",
    "\n",
    "start_time = start_datetime\n",
    "end_time = end_datetime\n",
    "\n",
    "filtered_df2 = psg_date_segment.loc[start_time:end_time]\n",
    "# Make a df subset with all the columns except: \n",
    "# 'ECG', 'Thor', 'Movement Time (MT)', 'Wakefulness (W)', 'NREM Sleep Stage 1 (N1)', \n",
    "# 'REM Sleep', 'NREM Sleep Stage 2 (N2)', 'NREM Sleep Stage 3 (N3)'\n",
    "columns_to_drop = ['Movement Time (MT)', 'Wakefulness (W)', 'NREM Sleep Stage 1 (N1)', 'REM Sleep', 'NREM Sleep Stage 2 (N2)', 'NREM Sleep Stage 3 (N3)']\n",
    "df_subset2 = filtered_df2.drop(columns=columns_to_drop)\n",
    "print(df_subset2.head())\n",
    "\n",
    "plt.figure(figsize=(30, 6))  # Set figure size for better readability\n",
    "plt.plot(df_subset2['Thor'], label='RSP')  # Plot the ECG column\n",
    "plt.title('RSP Over Time')  # Title of the plot\n",
    "plt.xlabel('Index')  # X-axis label\n",
    "plt.ylabel('RPS Value')  # Y-axis label\n",
    "plt.legend()  # Show legend\n",
    "plt.show()  # Display the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obstructive Apnea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_type = 'Obstructive Apnea'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pnea_df = df_sleep_events[df_sleep_events['Name'].str.contains(event_type)]\n",
    "pnea_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare the data (Time window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2024-06-21 03:33:36.600\t2024-06-21 03:33:54.830\n",
    "start_datetime = datetime(2024, 6, 21, 4, 10, 00)\n",
    "end_datetime = datetime(2024, 6, 21, 4, 11, 30) \n",
    "\n",
    "start_time = start_datetime\n",
    "end_time = end_datetime\n",
    "\n",
    "filtered_df = psg_date_segment.loc[start_time:end_time]\n",
    "# Make a df subset with all the columns except: \n",
    "# 'ECG', 'Thor', 'Movement Time (MT)', 'Wakefulness (W)', 'NREM Sleep Stage 1 (N1)', \n",
    "# 'REM Sleep', 'NREM Sleep Stage 2 (N2)', 'NREM Sleep Stage 3 (N3)'\n",
    "columns_to_drop = ['Movement Time (MT)', 'Wakefulness (W)', 'NREM Sleep Stage 1 (N1)', 'REM Sleep', 'NREM Sleep Stage 2 (N2)', 'NREM Sleep Stage 3 (N3)']\n",
    "df_subset = filtered_df.drop(columns=columns_to_drop)\n",
    "print(df_subset.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find Events (Auto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process and plot the events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage: \n",
    "non_zero_columns = check_non_zero_columns(psg_date_segment, start_datetime, end_datetime)\n",
    "print(non_zero_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define events for specific event types (label value above threshold: 0)\n",
    "# , df_subset['Limb Movement (Right)']\n",
    "events3 = nk.events_find(df_subset[event_type], threshold=0, threshold_keep='above')\n",
    "print(events3)\n",
    "plot = nk.events_plot(events3, df_subset[['ECG', 'Thor']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events4 = nk.events_find(df_subset[event_type], threshold=0, threshold_keep='below')\n",
    "print(events4)\n",
    "plot = nk.events_plot(events4, df_subset[['ECG', 'Thor']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the arrays\n",
    "combined_onset = np.concatenate((events3['onset'], events4['onset']))\n",
    "combined_duration = np.concatenate((events3['duration'], events4['duration']))\n",
    "\n",
    "# Sort the arrays based on the combined onset values\n",
    "sorted_indices = np.argsort(combined_onset)\n",
    "sorted_onset = combined_onset[sorted_indices]\n",
    "sorted_duration = combined_duration[sorted_indices]\n",
    "sorted_label = [str(i+1) for i in range(len(sorted_onset))]\n",
    "\n",
    "\n",
    "# Create the merged dictionary\n",
    "merged_events = {\n",
    "    'onset': sorted_onset,\n",
    "    'duration': sorted_duration,\n",
    "    'label': sorted_label\n",
    "}\n",
    "\n",
    "print(merged_events)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = nk.events_plot(merged_events, df_subset[['ECG', 'Thor']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process the Signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset[\"ECG\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(30, 6))  # Set figure size for better readability\n",
    "plt.plot(df_subset['ECG'], label='ECG')  # Plot the ECG column\n",
    "plt.title('ECG Over Time')  # Title of the plot\n",
    "plt.xlabel('Index')  # X-axis label\n",
    "plt.ylabel('ECG Value')  # Y-axis label\n",
    "plt.legend()  # Show legend\n",
    "plt.show()  # Display the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30, 6))  # Set figure size for better readability\n",
    "plt.plot(df_subset['Thor'], label='RSP')  # Plot the ECG column\n",
    "plt.title('RSP Over Time')  # Title of the plot\n",
    "plt.xlabel('Index')  # X-axis label\n",
    "plt.ylabel('RPS Value')  # Y-axis label\n",
    "plt.legend()  # Show legend\n",
    "plt.show()  # Display the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the signal\n",
    "data_clean, info_rsp = nk.bio_process(\n",
    "                                  ecg=df_subset[\"ECG\"],\n",
    "                                  rsp=df_subset[\"Thor\"],\n",
    "                                  keep=df_subset[event_type],  \n",
    "                                  sampling_rate=1024)\n",
    "\n",
    "# Visualize some of the channels\n",
    "data_clean[[\"ECG_Rate\", \"RSP_Rate\", event_type]].plot(subplots=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and plot epochs\n",
    "epochs = nk.epochs_create(data_clean, merged_events, sampling_rate=1024, epochs_start=-10, epochs_end=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch_index, epoch in epochs.items():\n",
    "    for attribute, value in epoch.items():\n",
    "        print(f\"  {attribute}\")\n",
    "    print(\"\\n\")  # Adds a newline for better readability between epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through epoch data\n",
    "for epoch in epochs.values():\n",
    "    # Plot scaled signals, \"Stage Code\"\n",
    "    nk.signal_plot(epoch[['ECG_Rate', 'RSP_Rate']], \n",
    "                   title=epoch['Label'].values[0],  # Extract condition name\n",
    "                   standardize=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interval Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = nk.events_plot(merged_events,data_clean[[\"ECG_Rate\", \"RSP_Rate\", event_type]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(merged_events)\n",
    "epochs = nk.epochs_create(data_clean, merged_events, sampling_rate=1024, epochs_start=0, epochs_end='from_events')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nk.ecg_intervalrelated(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nk.rsp_intervalrelated(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pnea_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2024-06-21 03:33:36.600\t2024-06-21 03:33:54.830\n",
    "start_datetime = datetime(2024, 6, 21, 4, 10, 00)\n",
    "end_datetime = datetime(2024, 6, 21, 4, 11, 30) \n",
    "\n",
    "start_time = start_datetime\n",
    "end_time = end_datetime\n",
    "\n",
    "filtered_df2 = psg_date_segment.loc[start_time:end_time]\n",
    "# Make a df subset with all the columns except: \n",
    "# 'ECG', 'Thor', 'Movement Time (MT)', 'Wakefulness (W)', 'NREM Sleep Stage 1 (N1)', \n",
    "# 'REM Sleep', 'NREM Sleep Stage 2 (N2)', 'NREM Sleep Stage 3 (N3)'\n",
    "columns_to_drop = ['Movement Time (MT)', 'Wakefulness (W)', 'NREM Sleep Stage 1 (N1)', 'REM Sleep', 'NREM Sleep Stage 2 (N2)', 'NREM Sleep Stage 3 (N3)']\n",
    "df_subset2 = filtered_df2.drop(columns=columns_to_drop)\n",
    "print(df_subset2.head())\n",
    "\n",
    "plt.figure(figsize=(30, 6))  # Set figure size for better readability\n",
    "plt.plot(df_subset2['Thor'], label='RSP')  # Plot the ECG column\n",
    "plt.title('RSP Over Time')  # Title of the plot\n",
    "plt.xlabel('Index')  # X-axis label\n",
    "plt.ylabel('RPS Value')  # Y-axis label\n",
    "plt.legend()  # Show legend\n",
    "plt.show()  # Display the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 1: Create 3D dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = xml_processor.events['Name'].values\n",
    "unique_events = np.unique(events)\n",
    "unique_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(psg_processor.start_datetime)\n",
    "print(psg_processor.end_datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_type = 'Obstructive Apnea'\n",
    "event_df = df_sleep_events[df_sleep_events['Name'].str.contains(event_type)]\n",
    "event_df\n",
    "\n",
    "# 2024-06-21 03:33:36.600\t2024-06-21 03:33:54.830\n",
    "start_datetime = datetime(2024, 6, 20, 22, 2, 35)\n",
    "end_datetime = datetime(2024, 6, 21, 5, 54, 58) \n",
    "\n",
    "start_time = start_datetime\n",
    "end_time = end_datetime\n",
    "\n",
    "filtered_df = psg_date_segment.loc[start_time:end_time]\n",
    "# Make a df subset with all the columns except: \n",
    "# 'ECG', 'Thor', 'Movement Time (MT)', 'Wakefulness (W)', 'NREM Sleep Stage 1 (N1)', \n",
    "# 'REM Sleep', 'NREM Sleep Stage 2 (N2)', 'NREM Sleep Stage 3 (N3)'\n",
    "columns_to_drop = ['Movement Time (MT)', 'Wakefulness (W)', 'NREM Sleep Stage 1 (N1)', 'REM Sleep', 'NREM Sleep Stage 2 (N2)', 'NREM Sleep Stage 3 (N3)']\n",
    "df_subset = filtered_df.drop(columns=columns_to_drop)\n",
    "print(df_subset.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the dictionary to hold the 3-level structure\n",
    "structured_data = {}\n",
    "\n",
    "# Ensure the 'Start' and 'End' times in xml_processor.events are in datetime format\n",
    "# Assuming 'Start' and 'End' are already in datetime or timedelta format\n",
    "events_df = xml_processor.events\n",
    "\n",
    "# Iterate over each unique event type\n",
    "for event_type in events_df['Name'].unique():\n",
    "    # Filter the events of this type\n",
    "    event_occurrences = events_df[events_df['Name'] == event_type]\n",
    "    \n",
    "    # Initialize a list to hold all occurrences for this event type\n",
    "    occurrences_list = []\n",
    "    \n",
    "    # Iterate over each occurrence of this event type\n",
    "    for _, event_row in event_occurrences.iterrows():\n",
    "        start_time = event_row['Start']\n",
    "        end_time = event_row['End']\n",
    "        \n",
    "        # Extract the relevant rows from df_subset for ECG and Thor within the time range\n",
    "        relevant_data = df_subset[(df_subset.index >= start_time) & (df_subset.index <= end_time)][['ECG', 'Thor']]\n",
    "        \n",
    "        # Add the relevant data for this occurrence\n",
    "        occurrences_list.append({\n",
    "            'Start': start_time,\n",
    "            'End': end_time,\n",
    "            'ECG': relevant_data['ECG'].values,\n",
    "            'Thor': relevant_data['Thor'].values,\n",
    "            'Time': relevant_data.index\n",
    "        })\n",
    "    \n",
    "    # Store the list of occurrences for this event type\n",
    "    structured_data[event_type] = occurrences_list\n",
    "\n",
    "# Example: Access data for a specific event type\n",
    "# structured_data['Mixed Apnea'] will give you the occurrences and corresponding ECG/Thor data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_event_data(event_type):\n",
    "    # Check if the event_type exists in the structured_data\n",
    "    if event_type not in structured_data:\n",
    "        print(f\"Event type '{event_type}' not found in structured data.\")\n",
    "        return\n",
    "    \n",
    "    # Retrieve all occurrences for the given event type\n",
    "    event_data = structured_data[event_type]\n",
    "    \n",
    "    # Iterate over each occurrence and plot ECG and Thor data\n",
    "    for i, occurrence in enumerate(event_data):\n",
    "        # Create subplots for ECG and Thor\n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(18, 4))  # 2 rows, 1 column\n",
    "\n",
    "        # Plot ECG\n",
    "        ax1.plot(occurrence['Time'], occurrence['ECG'], label='ECG')\n",
    "        ax1.set_title(f'{event_type} - ECG (# {i + 1})')\n",
    "        ax1.set_xlabel('Time')\n",
    "        ax1.set_ylabel('ECG Value')\n",
    "        ax1.legend()\n",
    "\n",
    "        # Plot Thor\n",
    "        ax2.plot(occurrence['Time'], occurrence['Thor'], label='Thor')\n",
    "        ax2.set_title(f'{event_type} - Thor (# {i + 1})')\n",
    "        ax2.set_xlabel('Time')\n",
    "        ax2.set_ylabel('Thor Value')\n",
    "        ax2.legend()\n",
    "\n",
    "        plt.tight_layout()  # Adjust spacing between subplots\n",
    "        plt.show()  # Display the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Central Apnea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "event_type_name = \"Central Apnea\"  # Replace with the event type you want to plot\n",
    "plot_event_data(event_type_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obstructive Apnea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "event_type_name = \"Obstructive Apnea\"  # Replace with the event type you want to plot\n",
    "plot_event_data(event_type_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hypopnea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "event_type_name = \"Hypopnea\"  # Replace with the event type you want to plot\n",
    "plot_event_data(event_type_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psg_date_segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 2: Identify events at whole-night scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sleep_stages = xml_processor.sleep_stages\n",
    "\n",
    "start_datetime = psg_processor.start_datetime\n",
    "end_datetime = psg_processor.end_datetime\n",
    "\n",
    "filtered_df = psg_date_segment.loc[start_datetime:end_datetime]\n",
    "df_subset = filtered_df[['ECG', 'Thor']]\n",
    "print(df_subset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_sleep_stage_column(df_subset, df_sleep_stages):\n",
    "    # Create a new column in df_subset to store the sleep stage code\n",
    "    df_subset['Stage Code'] = None\n",
    "\n",
    "    # Iterate over each row in df_sleep_stages\n",
    "    for index, row in df_sleep_stages.iterrows():\n",
    "        stage_start_time = row['Start Time']\n",
    "        stage_end_time = stage_start_time + timedelta(seconds=30)\n",
    "\n",
    "        # Find the rows in df_subset that fall within the current sleep stage interval\n",
    "        mask = (df_subset.index >= stage_start_time) & (df_subset.index < stage_end_time)\n",
    "        \n",
    "        # Assign the sleep stage code to the corresponding rows in df_subset\n",
    "        df_subset.loc[mask, 'Stage Code'] = row['Sleep Stage Code']\n",
    "        # df_subset.loc[mask, 'Sleep Stage'] = row['Sleep Stage']\n",
    "        # df_subset.loc[mask, 'Time (seconds)'] = row['Time (seconds)']\n",
    "        # df_subset.loc[mask, 'Duration'] = row['Duration']\n",
    "    \n",
    "    return df_subset\n",
    "\n",
    "# Example usage\n",
    "# Apply the function to add the sleep stage column\n",
    "df_subset_with_sleep_stage = add_sleep_stage_column(df_subset, df_sleep_stages)\n",
    "\n",
    "print(df_subset_with_sleep_stage)\n",
    "print(df_subset_with_sleep_stage['Stage Code'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset_with_sleep_stage = df_subset_with_sleep_stage.dropna(subset=['Stage Code'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events1 = nk.events_find(df_subset_with_sleep_stage['Stage Code'], threshold=0, threshold_keep='below')\n",
    "events2 = nk.events_find(df_subset_with_sleep_stage['Stage Code'], threshold=0, threshold_keep='above')\n",
    "\n",
    "\n",
    "# Combine the arrays\n",
    "combined_onset = np.concatenate((events1['onset'], events2['onset']))\n",
    "combined_duration = np.concatenate((events1['duration'], events2['duration']))\n",
    "\n",
    "# Sort the arrays based on the combined onset values\n",
    "sorted_indices = np.argsort(combined_onset)\n",
    "sorted_onset = combined_onset[sorted_indices]\n",
    "sorted_duration = combined_duration[sorted_indices]\n",
    "sorted_label = [str(i+1) for i in range(len(sorted_onset))]\n",
    "\n",
    "\n",
    "# Create the merged dictionary\n",
    "merged_events = {\n",
    "    'onset': sorted_onset,\n",
    "    'duration': sorted_duration,\n",
    "    'label': sorted_label\n",
    "}\n",
    "\n",
    "print(merged_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the signal\n",
    "data_clean, info = nk.bio_process(ecg=df_subset_with_sleep_stage[\"ECG\"], \n",
    "                                  rsp=df_subset_with_sleep_stage[\"Thor\"],\n",
    "                                  keep=df_subset_with_sleep_stage[\"Stage Code\"],  \n",
    "                                  sampling_rate=1024)\n",
    "\n",
    "# Visualize some of the channels\n",
    "data_clean[[\"ECG_Rate\", \"RSP_Rate\", \"Stage Code\"]].plot(subplots=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = nk.events_plot(merged_events,data_clean[[\"ECG_Rate\", \"RSP_Rate\", \"Stage Code\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = nk.epochs_create(data_clean, merged_events, sampling_rate=1024, epochs_start=0, epochs_end='from_events')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nk.rsp_intervalrelated(epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nk.ecg_intervalrelated(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = nk.epochs_create(data_clean, merged_events, sampling_rate=1024, epochs_start=-30, epochs_end=30)\n",
    "# Iterate through epoch data\n",
    "for epoch in epochs.values():\n",
    "    # Plot scaled signals, \"Stage Code\"\n",
    "    nk.signal_plot(epoch[['ECG_Rate', 'RSP_Rate']], \n",
    "                   title=epoch['Label'].values[0],  # Extract condition name\n",
    "                   standardize=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_subset_with_sleep_stage.head())\n",
    "print(df_subset_with_sleep_stage['Stage Code'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df is your DataFrame and 'Stage Code' is the column you want to subset by\n",
    "\n",
    "# Get the unique stage codes\n",
    "unique_stage_codes = df_subset_with_sleep_stage['Stage Code'].unique()\n",
    "\n",
    "# Create a dictionary to hold the subsets\n",
    "stage_code_subsets = {}\n",
    "\n",
    "# Loop through each unique stage code and generate a subset\n",
    "for code in unique_stage_codes:\n",
    "    # Create a subset for the current stage code and sort by 'Time'\n",
    "    subset = df_subset_with_sleep_stage[df_subset_with_sleep_stage['Stage Code'] == code].sort_index(ascending=True)\n",
    "    \n",
    "    # Store the subset in the dictionary\n",
    "    stage_code_subsets[code] = subset\n",
    "\n",
    "# Now stage_code_subsets is a dictionary where the keys are stage codes\n",
    "# and the values are the corresponding DataFrames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage_0 = stage_code_subsets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean, info = nk.bio_process(ecg=stage_0[\"ECG\"], \n",
    "                                  rsp=stage_0[\"Thor\"],\n",
    "                                  keep=stage_0[\"Stage Code\"],  \n",
    "                                  sampling_rate=1024)\n",
    "\n",
    "# Visualize some of the channels\n",
    "data_clean[[\"ECG_Rate\", \"RSP_Rate\", \"Stage Code\"]].plot(subplots=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If df_subset has only one unique Stage Code, create an \"event\" that covers the entire duration\n",
    "events = {'onset': [0], 'duration': [len(df_subset)], 'label': [df_subset['Stage Code'].iloc[0]]}\n",
    "# Create a single epoch that covers the entire subset\n",
    "epochs = nk.epochs_create(data_clean, events, sampling_rate=1024, epochs_start=0, epochs_end=len(df_subset)/1024)\n",
    "\n",
    "# Perform the interval-related analysis\n",
    "rsp_results = nk.rsp_intervalrelated(epochs)\n",
    "ecg_results = nk.ecg_intervalrelated(epochs)\n",
    "\n",
    "# Output the results\n",
    "print(rsp_results)\n",
    "print(ecg_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean[[\"ECG_Rate\"]].plot(subplots=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean[[\"ECG_Rate\"]]\n",
    "mean_value = data_clean[\"ECG_Rate\"].mean()\n",
    "std_dev = data_clean[\"ECG_Rate\"].std()\n",
    "mean_value, std_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsp_results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg_results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rsp_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean[[\"RSP_Rate\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_sleep_stage_features(rsp_rate):\n",
    "    # Ensure the input is a numpy array\n",
    "    rsp_rate = np.array(rsp_rate)\n",
    "    \n",
    "    # 1. RSP_Rate_Mean (Mean Respiratory Rate)\n",
    "    rsp_rate_mean = np.mean(rsp_rate)\n",
    "    \n",
    "    # 2. RRV_LFHF (Low Frequency to High Frequency Ratio)\n",
    "    # Here, we will simulate LF and HF components. In practice, these would be derived from a frequency analysis\n",
    "    # Using an FFT (Fast Fourier Transform) or another spectral analysis method.\n",
    "    fft_values = np.fft.fft(rsp_rate)\n",
    "    freqs = np.fft.fftfreq(len(rsp_rate))\n",
    "    \n",
    "    # Define LF (0.04 - 0.15 Hz) and HF (0.15 - 0.4 Hz) ranges\n",
    "    lf_band = (0.04, 0.15)\n",
    "    hf_band = (0.15, 0.4)\n",
    "    \n",
    "    lf_power = np.sum(np.abs(fft_values[(freqs >= lf_band[0]) & (freqs <= lf_band[1])])**2)\n",
    "    hf_power = np.sum(np.abs(fft_values[(freqs >= hf_band[0]) & (freqs <= hf_band[1])])**2)\n",
    "    \n",
    "    rrv_lfhf = lf_power / hf_power if hf_power != 0 else np.nan\n",
    "    \n",
    "    # 3. RSP_RVT (Respiratory Volume per Time)\n",
    "    # This is usually calculated using a more complex method involving the integration of the flow signal.\n",
    "    # For simplicity, we'll assume it relates to the amplitude of RSP rate changes.\n",
    "    rsp_rvt = np.std(rsp_rate)\n",
    "    \n",
    "    # 4. RRV_RMSSD (Root Mean Square of the Successive Differences)\n",
    "    diff_rsp_rate = np.diff(rsp_rate)\n",
    "    rrv_rmssd = np.sqrt(np.mean(diff_rsp_rate ** 2))\n",
    "    \n",
    "    # 5. RSP_Phase_Duration_Ratio (Inspiration/Expiration Duration Ratio)\n",
    "    # Assuming inspiration and expiration phases are marked by increases and decreases in RSP rate,\n",
    "    # We could approximate this by finding the ratio of mean positive changes to mean negative changes.\n",
    "    inspiration_durations = rsp_rate[diff_rsp_rate > 0]\n",
    "    expiration_durations = rsp_rate[diff_rsp_rate < 0]\n",
    "    \n",
    "    insp_duration_mean = np.mean(inspiration_durations) if len(inspiration_durations) > 0 else np.nan\n",
    "    exp_duration_mean = np.mean(expiration_durations) if len(expiration_durations) > 0 else np.nan\n",
    "    \n",
    "    rsp_phase_duration_ratio = insp_duration_mean / exp_duration_mean if exp_duration_mean != 0 else np.nan\n",
    "    \n",
    "    # Return the features as a dictionary\n",
    "    features = {\n",
    "        'RSP_Rate_Mean': rsp_rate_mean,\n",
    "        'RRV_LFHF': rrv_lfhf,\n",
    "        'RSP_RVT': rsp_rvt,\n",
    "        'RRV_RMSSD': rrv_rmssd,\n",
    "        'RSP_Phase_Duration_Ratio': rsp_phase_duration_ratio\n",
    "    }\n",
    "    \n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_sleep_stage_features(data_clean[[\"RSP_Rate\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean[[\"RSP_Rate\"]].plot(subplots=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch_index, epoch in epochs.items():\n",
    "    for attribute, value in epoch.items():\n",
    "        print(f\"  {attribute}\")\n",
    "    print(\"\\n\")  # Adds a newline for better readability between epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 3: Resample PSG data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import resample_poly\n",
    "\n",
    "def resample_data(data, original_freq, target_freq):\n",
    "    \"\"\"\n",
    "    Resamples the signal data to the target frequency.\n",
    "    \n",
    "    Parameters:\n",
    "    - data: DataFrame or NumPy array containing the data. If DataFrame, it should have signal channels and a datetime index ('Time').\n",
    "    - original_freq: The original sampling frequency (Hz).\n",
    "    - target_freq: The target sampling frequency (Hz).\n",
    "    \n",
    "    Returns:\n",
    "    - Resampled data: DataFrame or NumPy array, depending on input type.\n",
    "    \"\"\"\n",
    "    # Determine resampling factor\n",
    "    if original_freq == target_freq:\n",
    "        return data  # No resampling needed\n",
    "\n",
    "    resample_factor = original_freq / target_freq\n",
    "\n",
    "    if isinstance(data, pd.DataFrame):\n",
    "        # Resampling for DataFrame\n",
    "        if original_freq > target_freq:\n",
    "            # Downsampling\n",
    "            resampled_data = data.apply(lambda x: resample_poly(x, up=1, down=int(resample_factor)))\n",
    "        else:\n",
    "            # Upsampling\n",
    "            resampled_data = data.apply(lambda x: resample_poly(x, up=int(target_freq/original_freq), down=1))\n",
    "        \n",
    "        # # Resample the 'Time' index\n",
    "        # new_time_index = pd.date_range(start=data.index[0], periods=len(resampled_data), freq=f'{1000/target_freq}ms')\n",
    "        # resampled_data.index = new_time_index\n",
    "        \n",
    "        # Resample the 'Time' index using interpolation\n",
    "        resampled_data.index = pd.date_range(start=data.index[0], end=data.index[-1], periods=len(resampled_data))\n",
    "\n",
    "\n",
    "        return resampled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (20, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Start Timestamp: {psg_processor.start_datetime}, End Timestamp: {psg_processor.end_datetime}\")\n",
    "print(psg_processor.sampling_rate)\n",
    "\n",
    "# Extract data between two timestamps\n",
    "start_datetime = datetime(2024, 6, 20, 22, 10, 33) # Replace with your actual start datetime\n",
    "end_datetime = datetime(2024, 6, 20, 22, 11, 33)  # Replace with your actual end datetime\n",
    "\n",
    "print(f\"Start Timestamp: {start_datetime}, End Timestamp: {end_datetime}\")  # Print the start and end timestamps of the extracted data\n",
    "psg_date_segment = psg_processor.extract_segment_by_timestamp(start_datetime, end_datetime)\n",
    "psg_date_segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "psg_resampled = resample_data(psg_date_segment, original_freq=psg_processor.sampling_rate, target_freq=64)\n",
    "psg_resampled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the original dataset\n",
    "plt.plot(psg_date_segment.index, psg_date_segment['ECG'], label='Original')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('ECG')\n",
    "plt.title('Original ECG Data')\n",
    "plt.legend()\n",
    "\n",
    "# Plot the downsampled dataset\n",
    "plt.plot(psg_resampled.index, psg_resampled['ECG'], label='Downsampled')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('ECG')\n",
    "plt.title('Downsampled ECG Data')\n",
    "plt.legend()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the original dataset\n",
    "plt.plot(psg_date_segment.index, psg_date_segment['Thor'], label='Original')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Thor')\n",
    "plt.title('Original Thor Data')\n",
    "plt.legend()\n",
    "\n",
    "# Plot the downsampled dataset\n",
    "plt.plot(psg_resampled.index, psg_resampled['Thor'], label='Downsampled')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Thor')\n",
    "plt.title('Downsampled Thor Data')\n",
    "plt.legend()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 4: Resample CW Radar Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cw_radar import *\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# Example usage of CWDataProcessor\n",
    "file_path = \"/opt/data/private/ZhouWenren/SleepLab/cw_radar/radar20240620220948433561.csv\"\n",
    "radar_sample_rate = 1002\n",
    "\n",
    "# Create an instance of the CWDataProcessor\n",
    "cw_processor = CWDataProcessor(file_path, radar_sample_rate)\n",
    "\n",
    "# Load the data from the specified file\n",
    "cw_processor.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = cw_processor.data\n",
    "d_time = d['timestamp'].iloc[-1].strftime('%Y-%m-%d %H:%M:%S')\n",
    "print(f\"End time: {d_time}\")\n",
    "# Start Timestamp: 2024-06-20 22:02:34, End Timestamp: 2024-06-21 05:54:58.999023\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_subset = extract_data_subset(cw_processor.data, start_datetime, end_datetime)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_sig = cw_processor.process_signal(data_subset)\n",
    "radar_resampled = resample_data(processed_sig, original_freq=radar_sample_rate, target_freq=64)\n",
    "radar_resampled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the original dataset\n",
    "plt.figure(figsize=(20, 3))\n",
    "plt.plot(processed_sig, label='Original')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Radar Signal')\n",
    "plt.title('Original Radar Signal')\n",
    "plt.legend()\n",
    "\n",
    "# Plot the downsampled dataset\n",
    "plt.figure(figsize=(20, 3))\n",
    "plt.plot(radar_resampled, label='Downsampled')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Radar Signal')\n",
    "plt.title('Downsampled Radar Signal')\n",
    "plt.legend()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "radar_resampled['processed_signal'] = lowpass_filter(radar_sample_rate, radar_resampled['processed_signal'], 10, order=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure and three subplots vertically\n",
    "fig, axs = plt.subplots(5, 1, figsize=(20, 10))\n",
    "\n",
    "# Plot the downsampled ECG data\n",
    "axs[0].plot(psg_resampled.index, psg_resampled['ECG'], label='Downsampled')\n",
    "axs[0].set_xlabel('Time')\n",
    "axs[0].set_ylabel('ECG')\n",
    "axs[0].set_title('Downsampled ECG Data')\n",
    "axs[0].legend()\n",
    "\n",
    "# Plot the downsampled Thor data\n",
    "axs[1].plot(psg_resampled.index, psg_resampled['Thor'], label='Downsampled')\n",
    "axs[1].set_xlabel('Time')\n",
    "axs[1].set_ylabel('Thor')\n",
    "axs[1].set_title('Downsampled Thor Data')\n",
    "axs[1].legend()\n",
    "\n",
    "# Plot the downsampled ECG data again\n",
    "axs[2].plot(radar_resampled.index, radar_resampled['processed_signal'], label='Downsampled')\n",
    "axs[2].set_xlabel('Time')\n",
    "axs[2].set_ylabel('Radar Signal')\n",
    "axs[2].set_title('Downsampled Radar Signal')\n",
    "axs[2].legend()\n",
    "\n",
    "# Plot the downsampled Abdo data\n",
    "axs[3].plot(psg_resampled.index, psg_resampled['Abdo'], label='Downsampled')\n",
    "axs[3].set_xlabel('Time')\n",
    "axs[3].set_ylabel('Abdo')\n",
    "axs[3].set_title('Downsampled Abdo Data')\n",
    "axs[3].legend()\n",
    "\n",
    "# Plot the downsampled SpO2 data\n",
    "axs[4].plot(psg_resampled.index, psg_resampled['SpO2'], label='Downsampled')\n",
    "axs[4].set_xlabel('Time')\n",
    "axs[4].set_ylabel('SpO2')\n",
    "axs[4].set_title('Downsampled SpO2 Data')\n",
    "axs[4].legend()\n",
    "\n",
    "# Adjust the spacing between subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(psg_date_segment)\n",
    "print(data_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(psg_resampled)\n",
    "print(radar_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a common time grid (using the minimum and maximum timestamps)\n",
    "common_time_index = pd.date_range(\n",
    "    start=max(psg_resampled.index.min(), radar_resampled.index.min()), \n",
    "    end=min(psg_resampled.index.max(), radar_resampled.index.max()), \n",
    "    freq='15.625ms'  # Adjust frequency as necessary, here it matches 64Hz\n",
    ")\n",
    "\n",
    "# Reindex and interpolate both dataframes to this common time grid\n",
    "psg_aligned = psg_resampled.reindex(common_time_index, method='nearest')\n",
    "radar_aligned = radar_resampled.reindex(common_time_index, method='nearest')\n",
    "\n",
    "# Now psg_aligned and radar_aligned have the same time index and can be compared\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the aligned signals\n",
    "plt.figure(figsize=(20, 3))\n",
    "plt.plot(psg_aligned['Thor'], label='Aligned PSG Signal')\n",
    "plt.plot(radar_aligned, label='Aligned Radar Signal')\n",
    "# plt.plot(radar_resampled, label='Aligned & Low pass Filtered Radar Signal')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Signal Amplitude')\n",
    "plt.title('Aligned Signals After Cross-Correlation')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cross_correlation_time_alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# def cross_correlation_time_alignment(signal1, signal2, sample_rate):\n",
    "#     \"\"\"\n",
    "#     Aligns two signals by calculating the cross-correlation and shifting one signal to match the other.\n",
    "\n",
    "#     Parameters:\n",
    "#     - signal1, signal2: Pandas Series with timestamps as index.\n",
    "#     - sample_rate: The sample rate of the signals (in Hz).\n",
    "\n",
    "#     Returns:\n",
    "#     - aligned_signal1, aligned_signal2: Aligned signals as Pandas Series.\n",
    "#     \"\"\"\n",
    "#     # Ensure both signals have the same length\n",
    "#     min_length = min(len(signal1), len(signal2))\n",
    "#     signal1 = signal1.iloc[:min_length]\n",
    "#     signal2 = signal2.iloc[:min_length]\n",
    "\n",
    "#     # Calculate cross-correlation\n",
    "#     correlation = np.correlate(signal1 - np.mean(signal1), signal2 - np.mean(signal2), mode='full')\n",
    "#     lag = np.argmax(correlation) - (len(signal1) - 1)\n",
    "    \n",
    "#     # Convert lag to time\n",
    "#     time_shift = lag / sample_rate\n",
    "    \n",
    "#     # Shift signal1 or signal2 based on the calculated time lag\n",
    "#     if lag > 0:\n",
    "#         aligned_signal1 = signal1.iloc[lag:].reset_index(drop=True)\n",
    "#         aligned_signal2 = signal2.iloc[:-lag].reset_index(drop=True)\n",
    "#     else:\n",
    "#         aligned_signal1 = signal1.iloc[:lag].reset_index(drop=True)\n",
    "#         aligned_signal2 = signal2.iloc[-lag:].reset_index(drop=True)\n",
    "    \n",
    "#     # Create a common time index for the aligned signals\n",
    "#     common_time_index = pd.date_range(\n",
    "#         start=signal1.index[0] + pd.to_timedelta(time_shift, unit='s'), \n",
    "#         periods=len(aligned_signal1), \n",
    "#         freq=f'{1000 / sample_rate}ms'\n",
    "#     )\n",
    "    \n",
    "#     aligned_signal1.index = common_time_index\n",
    "#     aligned_signal2.index = common_time_index\n",
    "    \n",
    "#     return aligned_signal1, aligned_signal2, time_shift\n",
    "\n",
    "# # Example usage:\n",
    "# # Assuming 'psg_resampled' and 'radar_resampled' are your Pandas Series with the same time index\n",
    "# psg_signal = psg_aligned['Thor']  # Replace with your actual signal column\n",
    "# radar_signal = radar_aligned['processed_signal']  # Replace with your actual signal column\n",
    "\n",
    "# aligned_psg_signal, aligned_radar_signal, time_shift = cross_correlation_time_alignment(\n",
    "#     psg_signal, radar_signal, sample_rate=64  # Replace 64 with your actual sample rate\n",
    "# )\n",
    "\n",
    "# print(f\"Time shift applied: {time_shift} seconds\")\n",
    "\n",
    "# # Plot the aligned signals\n",
    "# plt.figure(figsize=(20, 3))\n",
    "# plt.plot(aligned_psg_signal, label='Aligned PSG Signal')\n",
    "# plt.plot(aligned_radar_signal, label='Aligned Radar Signal')\n",
    "# # plt.plot(radar_aligned, label='Aligned Radar Signal')\n",
    "# # plt.plot(radar_resampled, label='original Radar Signal')\n",
    "# plt.xlabel('Time')\n",
    "# plt.ylabel('Signal Amplitude')\n",
    "# plt.title('Aligned Signals After Cross-Correlation')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 5: Prepare Dataframe for ML Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load PSG data\n",
    "psg_processor = PSGDataProcessor(PSG_FILE_PATH)\n",
    "psg_processor.load_data()\n",
    "\n",
    "# Load radar data\n",
    "radar_sample_rate = 1002\n",
    "cw_processor = CWDataProcessor(RADAR_FILE_PATH, radar_sample_rate)\n",
    "cw_processor.load_data()\n",
    "\n",
    "# Load XML data\n",
    "xml_processor = XMLProcessor(XML_FILE_PATH, psg_processor.start_datetime)\n",
    "xml_processor.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data between two timestamps\n",
    "# start_datetime = datetime(2024, 6, 20, 22, 10, 33) # Replace with your actual start datetime\n",
    "# end_datetime = datetime(2024, 6, 20, 22, 11, 33)  # Replace with your actual end datetime\n",
    "start_datetime = psg_processor.start_datetime\n",
    "end_datetime = psg_processor.end_datetime\n",
    "print(f\"Start Timestamp: {start_datetime}, End Timestamp: {end_datetime}\")  # Print the start and end timestamps of the extracted data\n",
    "psg_date_segment = psg_processor.extract_segment_by_timestamp()\n",
    "\n",
    "radar_data_subset = extract_data_subset(cw_processor.data, start_datetime, end_datetime)\n",
    "processed_sig = cw_processor.process_signal(radar_data_subset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psg_resampled = resample_data(psg_date_segment, original_freq=psg_processor.sampling_rate, target_freq=64)\n",
    "\n",
    "radar_resampled = resample_data(processed_sig, original_freq=radar_sample_rate, target_freq=64)\n",
    "radar_resampled['processed_signal'] = lowpass_filter(radar_sample_rate, radar_resampled['processed_signal'], 10, order=4)\n",
    "\n",
    "# Reindex and interpolate both dataframes to this common time grid\n",
    "psg_aligned, radar_aligned = align_to_common_time_grid(psg_resampled, radar_resampled, freq_hz=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the aligned signals\n",
    "plt.figure(figsize=(20, 3))\n",
    "plt.plot(psg_aligned['Thor'], label='Aligned PSG Signal')\n",
    "plt.plot(radar_aligned, label='Aligned & Low pass Filtered Radar Signal')\n",
    "# plt.plot(radar_resampled, label='Low pass Filtered Radar Signal')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Signal Amplitude')\n",
    "plt.title('Aligned Signals After Cross-Correlation')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_aligned_df = psg_aligned.copy()\n",
    "# Insert the 'processed_signal' column from radar_aligned at the specified position (0)\n",
    "merged_aligned_df.insert(0, 'processed_signal', radar_aligned)\n",
    "\n",
    "# Integrate the XML data with the merged aligned DataFrame\n",
    "merged_df = integrate(merged_aligned_df, xml_processor)\n",
    "print(merged_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the merged DataFrame to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file path and get the directory path and filename\n",
    "psg_file_path = PSG_FILE_PATH\n",
    "dir_path, filename = os.path.split(psg_file_path)\n",
    "modified_filename = 'full_merged_' + filename\n",
    "\n",
    "# Save as Pickle file for efficiency and compactness\n",
    "save_path = os.path.join(dir_path, modified_filename + '.pkl')\n",
    "merged_df.to_pickle(save_path)\n",
    "print(f\"Pickle file saved at: {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the merged data from the saved Pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/data/private/ZhouWenren/SleepLab/psg/2024-6-20jiangyifan.edf\n",
      "                            processed_signal       ECG          Thor  \\\n",
      "2024-06-20 22:09:48.435561          0.224244 -0.000216  1.927425e-08   \n",
      "2024-06-20 22:09:48.451186          0.222440 -0.000034  3.047855e-04   \n",
      "2024-06-20 22:09:48.466811          0.220095 -0.000044 -1.876485e-08   \n",
      "2024-06-20 22:09:48.482436          0.217193 -0.000036 -3.120671e-04   \n",
      "2024-06-20 22:09:48.498061          0.213721 -0.000029  1.834376e-08   \n",
      "\n",
      "                                Abdo       SpO2  Movement Time (MT)  \\\n",
      "2024-06-20 22:09:48.435561  0.437458  98.084992                   0   \n",
      "2024-06-20 22:09:48.451186  0.462226  98.083586                   0   \n",
      "2024-06-20 22:09:48.466811  0.478490  98.084993                   0   \n",
      "2024-06-20 22:09:48.482436  0.488376  98.086501                   0   \n",
      "2024-06-20 22:09:48.498061  0.493507  98.084992                   0   \n",
      "\n",
      "                            Wakefulness (W)  NREM Sleep Stage 1 (N1)  \\\n",
      "2024-06-20 22:09:48.435561                1                        0   \n",
      "2024-06-20 22:09:48.451186                1                        0   \n",
      "2024-06-20 22:09:48.466811                1                        0   \n",
      "2024-06-20 22:09:48.482436                1                        0   \n",
      "2024-06-20 22:09:48.498061                1                        0   \n",
      "\n",
      "                            REM Sleep  NREM Sleep Stage 2 (N2)  ...  Hypopnea  \\\n",
      "2024-06-20 22:09:48.435561          0                        0  ...         0   \n",
      "2024-06-20 22:09:48.451186          0                        0  ...         0   \n",
      "2024-06-20 22:09:48.466811          0                        0  ...         0   \n",
      "2024-06-20 22:09:48.482436          0                        0  ...         0   \n",
      "2024-06-20 22:09:48.498061          0                        0  ...         0   \n",
      "\n",
      "                            PLM (Left)  PLM (Right)  SpO2 artifact  \\\n",
      "2024-06-20 22:09:48.435561           0            0              0   \n",
      "2024-06-20 22:09:48.451186           0            0              0   \n",
      "2024-06-20 22:09:48.466811           0            0              0   \n",
      "2024-06-20 22:09:48.482436           0            0              0   \n",
      "2024-06-20 22:09:48.498061           0            0              0   \n",
      "\n",
      "                            Arousal (ARO SPONT)  Arousal (ARO PLM)  \\\n",
      "2024-06-20 22:09:48.435561                    0                  0   \n",
      "2024-06-20 22:09:48.451186                    0                  0   \n",
      "2024-06-20 22:09:48.466811                    0                  0   \n",
      "2024-06-20 22:09:48.482436                    0                  0   \n",
      "2024-06-20 22:09:48.498061                    0                  0   \n",
      "\n",
      "                            Arousal (ARO Limb)  Arousal (ARO RES)  \\\n",
      "2024-06-20 22:09:48.435561                   0                  0   \n",
      "2024-06-20 22:09:48.451186                   0                  0   \n",
      "2024-06-20 22:09:48.466811                   0                  0   \n",
      "2024-06-20 22:09:48.482436                   0                  0   \n",
      "2024-06-20 22:09:48.498061                   0                  0   \n",
      "\n",
      "                            Central Apnea  Obstructive Apnea  \n",
      "2024-06-20 22:09:48.435561              0                  0  \n",
      "2024-06-20 22:09:48.451186              0                  0  \n",
      "2024-06-20 22:09:48.466811              0                  0  \n",
      "2024-06-20 22:09:48.482436              0                  0  \n",
      "2024-06-20 22:09:48.498061              0                  0  \n",
      "\n",
      "[5 rows x 25 columns]\n"
     ]
    }
   ],
   "source": [
    "from constants import PSG_FILE_PATH, XML_FILE_PATH, SAVE_PATH, RADAR_FILE_PATH, FULL_SAVE_PATH\n",
    "import pandas as pd\n",
    "\n",
    "# Specify the path to the pickle file\n",
    "save_path = FULL_SAVE_PATH\n",
    "pickle_path = save_path # '/path/to/your/filename.pkl'\n",
    "merged_df = pd.read_pickle(pickle_path)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(merged_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['processed_signal', 'ECG', 'Thor', 'Abdo', 'SpO2', 'Movement Time (MT)',\n",
       "       'Wakefulness (W)', 'NREM Sleep Stage 1 (N1)', 'REM Sleep',\n",
       "       'NREM Sleep Stage 2 (N2)', 'NREM Sleep Stage 3 (N3)', 'Mixed Apnea',\n",
       "       'Limb Movement (Left)', 'Limb Movement (Right)', 'SpO2 desaturation',\n",
       "       'Hypopnea', 'PLM (Left)', 'PLM (Right)', 'SpO2 artifact',\n",
       "       'Arousal (ARO SPONT)', 'Arousal (ARO PLM)', 'Arousal (ARO Limb)',\n",
       "       'Arousal (ARO RES)', 'Central Apnea', 'Obstructive Apnea'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'segment_and_label' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Applying the function to segment and label the dataframe with time columns\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m segmented_df \u001b[38;5;241m=\u001b[39m \u001b[43msegment_and_label\u001b[49m(merged_df, segment_sec\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, freq_hz\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m)\n\u001b[1;32m      3\u001b[0m segmented_df \u001b[38;5;241m=\u001b[39m segmented_df[\u001b[38;5;241m~\u001b[39msegmented_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDominant Sleep Stage\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39misin([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNo Stage\u001b[39m\u001b[38;5;124m'\u001b[39m])]\n\u001b[1;32m      4\u001b[0m segmented_df\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'segment_and_label' is not defined"
     ]
    }
   ],
   "source": [
    "# Applying the function to segment and label the dataframe with time columns\n",
    "segmented_df = segment_and_label(merged_df, segment_sec=10, freq_hz=64)\n",
    "segmented_df = segmented_df[~segmented_df['Dominant Sleep Stage'].isin(['No Stage'])]\n",
    "segmented_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def segment_and_label_as_nd_cubes(df, segment_sec=10, freq_hz=64, label_type='stage', input_signals=['processed_signal', 'ECG', 'Thor', 'Abdo', 'SpO2']):\n",
    "    \"\"\"\n",
    "    Segments the given DataFrame into n-dimensional cubes for each segment and assigns labels based on label_type.\n",
    "    \n",
    "    Parameters:\n",
    "    - df (pandas.DataFrame): The DataFrame containing the data to be segmented.\n",
    "    - segment_sec (int): The duration of each segment in seconds. Default is 10.\n",
    "    - freq_hz (int): The frequency of the data in Hz. Default is 64.\n",
    "    - label_type (str): The type of label to attach ('stage' for sleep stage, 'event' for sleep event). Default is 'stage'.\n",
    "    \n",
    "    Returns:\n",
    "    - X (numpy.ndarray): A 3D array (n-dimensional cube) representing the data for each segment.\n",
    "    - y (numpy.ndarray): An array of labels for each segment.\n",
    "    \"\"\"\n",
    "    # Number of data points per segment\n",
    "    segment_size = segment_sec * freq_hz\n",
    "    \n",
    "    # Define columns for sleep stages and sleep events\n",
    "    sleep_stages = ['Movement Time (MT)', 'Wakefulness (W)', 'NREM Sleep Stage 1 (N1)', 'REM Sleep', \n",
    "                    'NREM Sleep Stage 2 (N2)', 'NREM Sleep Stage 3 (N3)']\n",
    "    sleep_events = ['Mixed Apnea', 'Limb Movement (Left)', 'Limb Movement (Right)', 'SpO2 desaturation', \n",
    "                    'Hypopnea', 'PLM (Left)', 'PLM (Right)', 'SpO2 artifact', 'Arousal (ARO SPONT)', \n",
    "                    'Arousal (ARO PLM)', 'Arousal (ARO Limb)', 'Arousal (ARO RES)', 'Central Apnea', \n",
    "                    'Obstructive Apnea']\n",
    "    \n",
    "    # Initialize lists to store the segmented data and labels\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    # Loop over the DataFrame in chunks of segment_size\n",
    "    for start in range(0, len(df), segment_size):\n",
    "        # Get current segment\n",
    "        segment = df.iloc[start:start + segment_size]\n",
    "        \n",
    "        # Check if segment size is not less than required (especially for the last segment)\n",
    "        if len(segment) < segment_size:\n",
    "            continue\n",
    "        \n",
    "        # Create a 2D array (segment_size x number_of_features) for the current segment\n",
    "        segment_array = segment[input_signals].values\n",
    "        \n",
    "        # Append the array to X\n",
    "        X.append(segment_array)\n",
    "        \n",
    "        # Determine the label based on label_type\n",
    "        if label_type == 'stage':\n",
    "            # Determine the dominant sleep stage (most frequent one)\n",
    "            sleep_stage_counts = segment[sleep_stages].sum()\n",
    "            dominant_sleep_stage = sleep_stage_counts.idxmax() if sleep_stage_counts.max() > 0 else 'No Stage'\n",
    "            y.append(dominant_sleep_stage)\n",
    "        elif label_type == 'event':\n",
    "            # Determine the dominant sleep event (most frequent one)\n",
    "            sleep_event_counts = segment[sleep_events].sum()\n",
    "            dominant_sleep_event = sleep_event_counts.idxmax() if sleep_event_counts.max() > 0 else 'No Event'\n",
    "            y.append(dominant_sleep_event)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid label_type. Expected 'stage' or 'event'.\")\n",
    "    \n",
    "    # Convert X and y to numpy arrays\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Example usage:\n",
    "# Define columns for continuous data\n",
    "continuous_columns = ['processed_signal', 'ECG', 'Thor', 'Abdo', 'SpO2']\n",
    "\n",
    "# Apply the function to your DataFrame with 'stage' label\n",
    "X_stage, y_stage = segment_and_label_as_nd_cubes(merged_df, input_signals = continuous_columns, label_type='stage')\n",
    "\n",
    "# Apply the function to your DataFrame with 'event' label\n",
    "X_event, y_event = segment_and_label_as_nd_cubes(merged_df, input_signals = continuous_columns, label_type='event')\n",
    "\n",
    "print(\"Shape of X_stage:\", X_stage.shape)  # Should be (number of segments, segment_size, number of features)\n",
    "print(\"Shape of y_stage:\", y_stage.shape)  # Should be (number of segments,)\n",
    "print(\"Example label for 'stage' (y_stage[0]):\", y_stage[0])\n",
    "\n",
    "print(\"Shape of X_event:\", X_event.shape)  # Should be (number of segments, segment_size, number of features)\n",
    "print(\"Shape of y_event:\", y_event.shape)  # Should be (number of segments,)\n",
    "print(\"Example label for 'event' (y_event[0]):\", y_event[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 6: ML Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Mean value for continuous data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the segmented DataFrame\n",
    "df = segmented_df.copy()\n",
    "\n",
    "# Encode target variable\n",
    "label_encoder = LabelEncoder()\n",
    "df['Dominant Sleep Stage'] = label_encoder.fit_transform(df['Dominant Sleep Stage'])\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "features = ['processed_signal', 'ECG', 'Thor', 'Abdo', 'SpO2']\n",
    "X = df[features]\n",
    "y = df['Dominant Sleep Stage']\n",
    "\n",
    "# Split the data into training and testing sets (80-20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Initialize the Random Forest classifier\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "# Train the model on the training data\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Classification report and confusion matrix\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Visualize the confusion matrix\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use n-D structure for continuous data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define columns for continuous data\n",
    "continuous_columns = ['processed_signal', 'SpO2']\n",
    "\n",
    "# Apply the function to your DataFrame with 'stage' label\n",
    "X_stage, y_stage = segment_and_label_as_nd_cubes(merged_df, input_signals = continuous_columns, label_type='stage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue with the training and testing process using Random Forest\n",
    "# Encode target variable\n",
    "label_encoder = LabelEncoder()\n",
    "y_stage_encoded = label_encoder.fit_transform(y_stage)\n",
    "\n",
    "# Flatten X_stage for input into RandomForest (reshaping for (samples, features))\n",
    "n_samples, n_timesteps, n_features = X_stage.shape\n",
    "X_stage_flattened = X_stage.reshape(n_samples, n_timesteps * n_features)\n",
    "\n",
    "# Split the data into training and testing sets (80-20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_stage_flattened, y_stage_encoded, test_size=0.2, random_state=42, stratify=y_stage_encoded)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Initialize the Random Forest classifier\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model on the training data\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Get the unique classes in y_test to match with target_names\n",
    "unique_classes_in_test = np.unique(y_test)\n",
    "\n",
    "# Create a mapping from label indices to label names for the test data only\n",
    "test_class_names = label_encoder.inverse_transform(unique_classes_in_test)\n",
    "\n",
    "# Generate classification report with dynamically matched target names\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred, target_names=test_class_names))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Visualize the confusion matrix with the correct labels\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues', xticklabels=test_class_names, yticklabels=test_class_names)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define columns for continuous data\n",
    "continuous_columns = ['processed_signal', 'Thor']\n",
    "\n",
    "# Apply the function to your DataFrame with 'stage' label\n",
    "X_stage, y_stage = segment_and_label_as_nd_cubes(merged_df, input_signals = continuous_columns, label_type='stage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue with the training and testing process using Random Forest\n",
    "# Encode target variable\n",
    "label_encoder = LabelEncoder()\n",
    "y_stage_encoded = label_encoder.fit_transform(y_stage)\n",
    "\n",
    "# Flatten X_stage for input into RandomForest (reshaping for (samples, features))\n",
    "n_samples, n_timesteps, n_features = X_stage.shape\n",
    "X_stage_flattened = X_stage.reshape(n_samples, n_timesteps * n_features)\n",
    "\n",
    "# Split the data into training and testing sets (80-20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_stage_flattened, y_stage_encoded, test_size=0.2, random_state=42, stratify=y_stage_encoded)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Initialize the Random Forest classifier\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model on the training data\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Get the unique classes in y_test to match with target_names\n",
    "unique_classes_in_test = np.unique(y_test)\n",
    "\n",
    "# Create a mapping from label indices to label names for the test data only\n",
    "test_class_names = label_encoder.inverse_transform(unique_classes_in_test)\n",
    "\n",
    "# Generate classification report with dynamically matched target names\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred, target_names=test_class_names))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Visualize the confusion matrix with the correct labels\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues', xticklabels=test_class_names, yticklabels=test_class_names)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define columns for continuous data\n",
    "continuous_columns = ['processed_signal']\n",
    "\n",
    "# Apply the function to your DataFrame with 'stage' label\n",
    "X_stage, y_stage = segment_and_label_as_nd_cubes(merged_df, input_signals = continuous_columns, label_type='stage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue with the training and testing process using Random Forest\n",
    "# Encode target variable\n",
    "label_encoder = LabelEncoder()\n",
    "y_stage_encoded = label_encoder.fit_transform(y_stage)\n",
    "\n",
    "# Flatten X_stage for input into RandomForest (reshaping for (samples, features))\n",
    "n_samples, n_timesteps, n_features = X_stage.shape\n",
    "X_stage_flattened = X_stage.reshape(n_samples, n_timesteps * n_features)\n",
    "\n",
    "# Split the data into training and testing sets (80-20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_stage_flattened, y_stage_encoded, test_size=0.2, random_state=42, stratify=y_stage_encoded)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Initialize the Random Forest classifier\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model on the training data\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Get the unique classes in y_test to match with target_names\n",
    "unique_classes_in_test = np.unique(y_test)\n",
    "\n",
    "# Create a mapping from label indices to label names for the test data only\n",
    "test_class_names = label_encoder.inverse_transform(unique_classes_in_test)\n",
    "\n",
    "# Generate classification report with dynamically matched target names\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred, target_names=test_class_names))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Visualize the confusion matrix with the correct labels\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues', xticklabels=test_class_names, yticklabels=test_class_names)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1D CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define columns for continuous data\n",
    "continuous_columns = ['processed_signal']\n",
    "\n",
    "# Apply the function to your DataFrame with 'stage' label\n",
    "X_stage, y_stage = segment_and_label_as_nd_cubes(merged_df, input_signals = continuous_columns, label_type='stage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Assuming X_stage and y_stage are defined as in your previous steps\n",
    "# Encode target variable\n",
    "label_encoder = LabelEncoder()\n",
    "y_stage_encoded = label_encoder.fit_transform(y_stage)\n",
    "\n",
    "# Convert X_stage to 3D format for CNN input\n",
    "# X_stage = np.expand_dims(X_stage, axis=-1)  # Add a channel dimension for CNN input\n",
    "\n",
    "# Split the data into training and testing sets (80-20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_stage, y_stage_encoded, test_size=0.2, random_state=42, stratify=y_stage_encoded)\n",
    "\n",
    "# Build a basic CNN model\n",
    "cnn_model = Sequential([\n",
    "    Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Dropout(0.25),\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(np.unique(y_stage_encoded)), activation='softmax')  # Output layer for classification\n",
    "])\n",
    "\n",
    "cnn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "cnn_model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = np.argmax(cnn_model.predict(X_test), axis=-1)\n",
    "\n",
    "# Get the unique classes in y_test to match with target_names\n",
    "unique_classes_in_test = np.unique(y_test)\n",
    "\n",
    "# Create a mapping from label indices to label names for the test data only\n",
    "test_class_names = label_encoder.inverse_transform(unique_classes_in_test)\n",
    "\n",
    "# Generate classification report with dynamically matched target names\n",
    "print(\"CNN Classification Report:\\n\", classification_report(y_test, y_pred, target_names=test_class_names))\n",
    "print(\"CNN Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Visualize the confusion matrix with the correct labels\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues', xticklabels=test_class_names, yticklabels=test_class_names)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate classification report with dynamically matched target names\n",
    "print(\"CNN Classification Report:\\n\", classification_report(y_test, y_pred, target_names=test_class_names))\n",
    "print(\"CNN Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define columns for continuous data\n",
    "continuous_columns = ['processed_signal', 'Thor', 'ECG']\n",
    "\n",
    "# Apply the function to your DataFrame with 'stage' label\n",
    "X_stage, y_stage = segment_and_label_as_nd_cubes(merged_df, input_signals = continuous_columns, label_type='stage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "# Assuming X_stage and y_stage are defined as in your previous steps\n",
    "# Encode target variable\n",
    "label_encoder = LabelEncoder()\n",
    "y_stage_encoded = label_encoder.fit_transform(y_stage)\n",
    "\n",
    "# Reshape X_stage to 4D for 2D CNN input (samples, height, width, channels)\n",
    "# Here, we use the second dimension (time steps) as \"height\" and the third dimension (features) as \"width\"\n",
    "# X_stage_reshaped = np.expand_dims(X_stage, axis=-1)  # Add a channel dimension for CNN input\n",
    "\n",
    "# Split the data into training and testing sets (80-20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_stage, y_stage_encoded, test_size=0.2, random_state=42, stratify=y_stage_encoded)\n",
    "\n",
    "# Build a basic 2D CNN model\n",
    "cnn_model = Sequential([\n",
    "    Conv2D(filters=32, kernel_size=(2, 2), activation='relu', input_shape=(X_train.shape[1], X_train.shape[2], 1)),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Dropout(0.25),\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(np.unique(y_stage_encoded)), activation='softmax')  # Output layer for classification\n",
    "])\n",
    "\n",
    "cnn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "cnn_model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = np.argmax(cnn_model.predict(X_test), axis=-1)\n",
    "\n",
    "# Get the unique classes in y_test to match with target_names\n",
    "unique_classes_in_test = np.unique(y_test)\n",
    "\n",
    "# Create a mapping from label indices to label names for the test data only\n",
    "test_class_names = label_encoder.inverse_transform(unique_classes_in_test)\n",
    "\n",
    "# Generate classification report with dynamically matched target names\n",
    "print(\"CNN Classification Report:\\n\", classification_report(y_test, y_pred, target_names=test_class_names))\n",
    "print(\"CNN Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Visualize the confusion matrix with the correct labels\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues', xticklabels=test_class_names, yticklabels=test_class_names)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate classification report with dynamically matched target names\n",
    "print(\"CNN Classification Report:\\n\", classification_report(y_test, y_pred, target_names=test_class_names))\n",
    "print(\"CNN Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define columns for continuous data\n",
    "continuous_columns = ['processed_signal', 'Thor', 'ECG', 'SpO2']\n",
    "\n",
    "# Apply the function to your DataFrame with 'stage' label\n",
    "X_stage, y_stage = segment_and_label_as_nd_cubes(merged_df, input_signals = continuous_columns, label_type='stage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Assuming X_stage and y_stage are defined as in your previous steps\n",
    "# Encode target variable\n",
    "label_encoder = LabelEncoder()\n",
    "y_stage_encoded = label_encoder.fit_transform(y_stage)\n",
    "\n",
    "# Reshape X_stage to ensure it's 3D for LSTM input (samples, timesteps, features)\n",
    "# Here, X_stage should already be 3D if prepared as time-series data (samples, time steps, features)\n",
    "# No need for reshaping if X_stage is already correctly shaped\n",
    "\n",
    "# Split the data into training and testing sets (80-20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_stage, y_stage_encoded, test_size=0.2, random_state=42, stratify=y_stage_encoded)\n",
    "\n",
    "# Build a basic LSTM RNN model\n",
    "rnn_model = Sequential([\n",
    "    LSTM(64, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True),  # LSTM layer with 64 units\n",
    "    Dropout(0.2),\n",
    "    LSTM(32, return_sequences=False),  # LSTM layer with 32 units\n",
    "    Dropout(0.2),\n",
    "    Dense(len(np.unique(y_stage_encoded)), activation='softmax')  # Output layer for classification\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "rnn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "rnn_model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = np.argmax(rnn_model.predict(X_test), axis=-1)\n",
    "\n",
    "# Create a mapping from label indices to label names for the test data only\n",
    "test_class_names = label_encoder.inverse_transform(np.unique(y_test))\n",
    "\n",
    "# Generate classification report with dynamically matched target names\n",
    "print(\"RNN Classification Report:\\n\", classification_report(y_test, y_pred, target_names=test_class_names))\n",
    "print(\"RNN Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Visualize the confusion matrix with the correct labels\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues', xticklabels=test_class_names, yticklabels=test_class_names)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "# from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# from sklearn.metrics import classification_report, confusion_matrix\n",
    "# from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "# # Assuming X_stage and y_stage are defined as in your previous steps\n",
    "# # Encode target variable\n",
    "# label_encoder = LabelEncoder()\n",
    "# y_stage_encoded = label_encoder.fit_transform(y_stage)\n",
    "\n",
    "# # Split the data into training and testing sets (80-20 split)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_stage, y_stage_encoded, test_size=0.2, random_state=42, stratify=y_stage_encoded)\n",
    "\n",
    "# # Define the model creation function\n",
    "# def create_model(units=64, dropout_rate=0.2):\n",
    "#     model = Sequential([\n",
    "#         LSTM(units, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True),\n",
    "#         Dropout(dropout_rate),\n",
    "#         LSTM(units // 2, return_sequences=False),\n",
    "#         Dropout(dropout_rate),\n",
    "#         Dense(len(np.unique(y_stage_encoded)), activation='softmax')  # Output layer for classification\n",
    "#     ])\n",
    "    \n",
    "#     model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "#     return model\n",
    "\n",
    "# # Wrap the model with KerasClassifier for scikit-learn\n",
    "# model = KerasClassifier(build_fn=create_model, epochs=10, batch_size=32, verbose=0)\n",
    "\n",
    "# # Define the grid of hyperparameters to search\n",
    "# param_grid = {\n",
    "#     'units': [32, 64, 128],  # Number of LSTM units\n",
    "#     'dropout_rate': [0.2, 0.3, 0.4],  # Dropout rates\n",
    "#     'epochs': [10, 20],  # Number of epochs\n",
    "#     'batch_size': [32, 64]  # Batch sizes\n",
    "# }\n",
    "\n",
    "# # Set up GridSearchCV\n",
    "# grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=3)\n",
    "\n",
    "# # Run the grid search\n",
    "# grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "# # Print the best score and the best parameters found\n",
    "# print(f\"Best Accuracy: {grid_result.best_score_:.4f}\")\n",
    "# print(\"Best Parameters:\", grid_result.best_params_)\n",
    "\n",
    "# # Use the best estimator to make predictions\n",
    "# best_model = grid_result.best_estimator_\n",
    "# y_pred = best_model.predict(X_test)\n",
    "\n",
    "# # Create a mapping from label indices to label names for the test data only\n",
    "# test_class_names = label_encoder.inverse_transform(np.unique(y_test))\n",
    "\n",
    "# # Generate classification report with dynamically matched target names\n",
    "# print(\"RNN Classification Report:\\n\", classification_report(y_test, y_pred, target_names=test_class_names))\n",
    "# print(\"RNN Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Print the best score and the best parameters found\n",
    "# print(f\"Best Accuracy: {grid_result.best_score_:.4f}\")\n",
    "# print(\"Best Parameters:\", grid_result.best_params_)\n",
    "\n",
    "# # Use the best estimator to make predictions\n",
    "# best_model = grid_result.best_estimator_\n",
    "# y_pred = best_model.predict(X_test)\n",
    "\n",
    "# # Create a mapping from label indices to label names for the test data only\n",
    "# test_class_names = label_encoder.inverse_transform(np.unique(y_test))\n",
    "\n",
    "# # Generate classification report with dynamically matched target names\n",
    "# print(\"RNN Classification Report:\\n\", classification_report(y_test, y_pred, target_names=test_class_names))\n",
    "# print(\"RNN Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_result\n",
    "# # {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'units': 128}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define columns for continuous data\n",
    "continuous_columns = ['processed_signal', 'Thor', 'ECG', 'SpO2']\n",
    "\n",
    "# Apply the function to your DataFrame with 'stage' label\n",
    "X_stage, y_stage = segment_and_label_as_nd_cubes(merged_df, input_signals = continuous_columns, label_type='stage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.models import Model\n",
    "# from tensorflow.keras.layers import Input, Dense, Embedding, LayerNormalization, Dropout\n",
    "# from tensorflow.keras.layers import MultiHeadAttention, Add, GlobalAveragePooling1D\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# from sklearn.metrics import classification_report, confusion_matrix\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# # Assuming X_stage and y_stage are defined and preprocessed as in your previous steps\n",
    "# # Encode target variable\n",
    "# label_encoder = LabelEncoder()\n",
    "# y_stage_encoded = label_encoder.fit_transform(y_stage)\n",
    "\n",
    "# # Reshape X_stage to ensure it's 3D (samples, timesteps, features)\n",
    "# # X_stage = np.expand_dims(X_stage, axis=-1)  # Add a channel dimension if not already present\n",
    "\n",
    "# # Split the data into training and testing sets (80-20 split)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_stage, y_stage_encoded, test_size=0.2, random_state=42, stratify=y_stage_encoded)\n",
    "\n",
    "# # Define the Transformer model\n",
    "# def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "#     # Normalization and Attention\n",
    "#     x = LayerNormalization(epsilon=1e-6)(inputs)\n",
    "#     x = MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)(x, x)\n",
    "#     x = Dropout(dropout)(x)\n",
    "#     res = Add()([x, inputs])\n",
    "\n",
    "#     # Feed Forward Part\n",
    "#     x = LayerNormalization(epsilon=1e-6)(res)\n",
    "#     x = Dense(ff_dim, activation=\"relu\")(x)\n",
    "#     x = Dropout(dropout)(x)\n",
    "#     x = Dense(inputs.shape[-1])(x)\n",
    "#     return Add()([x, res])\n",
    "\n",
    "# def build_transformer_model(input_shape, head_size, num_heads, ff_dim, num_transformer_blocks, mlp_units, dropout=0, mlp_dropout=0):\n",
    "#     inputs = Input(shape=input_shape)\n",
    "#     x = inputs\n",
    "#     for _ in range(num_transformer_blocks):\n",
    "#         x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
    "\n",
    "#     x = GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
    "#     for dim in mlp_units:\n",
    "#         x = Dense(dim, activation=\"relu\")(x)\n",
    "#         x = Dropout(mlp_dropout)(x)\n",
    "#     outputs = Dense(len(np.unique(y_stage_encoded)), activation=\"softmax\")(x)\n",
    "#     return Model(inputs, outputs)\n",
    "\n",
    "# input_shape = X_train.shape[1:]\n",
    "# model = build_transformer_model(\n",
    "#     input_shape,\n",
    "#     head_size=256,\n",
    "#     num_heads=4,\n",
    "#     ff_dim=4,\n",
    "#     num_transformer_blocks=4,\n",
    "#     mlp_units=[128],\n",
    "#     dropout=0.25,\n",
    "#     mlp_dropout=0.4,\n",
    "# )\n",
    "\n",
    "# model.compile(\n",
    "#     loss=\"sparse_categorical_crossentropy\",\n",
    "#     optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "#     metrics=[\"sparse_categorical_accuracy\"],\n",
    "# )\n",
    "\n",
    "# # Train the model\n",
    "# model.fit(\n",
    "#     X_train,\n",
    "#     y_train,\n",
    "#     validation_split=0.2,\n",
    "#     epochs=20,\n",
    "#     batch_size=32,\n",
    "# )\n",
    "\n",
    "# # Evaluate the model\n",
    "# y_pred = np.argmax(model.predict(X_test), axis=-1)\n",
    "\n",
    "# # Create a mapping from label indices to label names for the test data only\n",
    "# test_class_names = label_encoder.inverse_transform(np.unique(y_test))\n",
    "\n",
    "# # Generate classification report with dynamically matched target names\n",
    "# print(\"Transformer Classification Report:\\n\", classification_report(y_test, y_pred, target_names=test_class_names))\n",
    "# print(\"Transformer Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# # Visualize the confusion matrix with the correct labels\n",
    "# plt.figure(figsize=(10, 7))\n",
    "# sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues', xticklabels=test_class_names, yticklabels=test_class_names)\n",
    "# plt.xlabel('Predicted')\n",
    "# plt.ylabel('True')\n",
    "# plt.title('Confusion Matrix')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Check if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU is available. Here are the details:\")\n",
    "    print(\" - GPU Name:\", torch.cuda.get_device_name(0))\n",
    "    print(\" - Number of GPUs:\", torch.cuda.device_count())\n",
    "    print(\" - Current GPU:\", torch.cuda.current_device())\n",
    "else:\n",
    "    print(\"GPU is not available. Please check your CUDA installation.\")\n",
    "\n",
    "# Perform a simple computation to test the GPU\n",
    "a = torch.randn(1000, 1000, device='cuda')\n",
    "b = torch.randn(1000, 1000, device='cuda')\n",
    "\n",
    "print(\"Performing a matrix multiplication to test the GPU...\")\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "c = torch.matmul(a, b)\n",
    "\n",
    "print(\"Matrix multiplication completed.\")\n",
    "print(\"Time taken: {:.3f} seconds\".format(time.time() - start_time))\n",
    "print(\"Device used for computation:\", c.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.backends.cudnn.version())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Encode target variable\n",
    "label_encoder = LabelEncoder()\n",
    "y_stage_encoded = label_encoder.fit_transform(y_stage)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_stage_tensor = torch.tensor(X_stage, dtype=torch.float32).to(device)\n",
    "y_stage_tensor = torch.tensor(y_stage_encoded, dtype=torch.long).to(device)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_stage_tensor, y_stage_tensor, test_size=0.2, random_state=42, stratify=y_stage_encoded)\n",
    "\n",
    "# Create PyTorch datasets and loaders\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Define the Transformer model\n",
    "class TransformerClassifier(nn.Module):\n",
    "    def __init__(self, input_shape, head_size, num_heads, ff_dim, num_transformer_blocks, mlp_units, dropout=0.25, mlp_dropout=0.4):\n",
    "        super(TransformerClassifier, self).__init__()\n",
    "        self.num_transformer_blocks = num_transformer_blocks\n",
    "        self.attention_heads = nn.ModuleList([\n",
    "            nn.MultiheadAttention(embed_dim=head_size, num_heads=num_heads, dropout=dropout) for _ in range(num_transformer_blocks)\n",
    "        ])\n",
    "        self.norm_layers = nn.ModuleList([nn.LayerNorm(head_size) for _ in range(num_transformer_blocks)])\n",
    "        self.ff_layers = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(head_size, ff_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(ff_dim, head_size)\n",
    "            ) for _ in range(num_transformer_blocks)\n",
    "        ])\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(head_size, mlp_units[0]),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(mlp_dropout),\n",
    "            nn.Linear(mlp_units[0], len(np.unique(y_stage_encoded)))\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i in range(self.num_transformer_blocks):\n",
    "            attn_output, _ = self.attention_heads[i](x, x, x)\n",
    "            x = self.norm_layers[i](x + attn_output)\n",
    "            ff_output = self.ff_layers[i](x)\n",
    "            x = self.norm_layers[i](x + ff_output)\n",
    "        x = self.pool(x.transpose(1, 2)).squeeze(-1)\n",
    "        x = self.mlp(x)\n",
    "        return x\n",
    "\n",
    "input_shape = X_train.shape[1:]\n",
    "model = TransformerClassifier(\n",
    "    input_shape=input_shape[1],  # Assuming input_shape is (timesteps, features)\n",
    "    head_size=256,\n",
    "    num_heads=4,\n",
    "    ff_dim=4,\n",
    "    num_transformer_blocks=4,\n",
    "    mlp_units=[128],\n",
    "    dropout=0.25,\n",
    "    mlp_dropout=0.4,\n",
    ").to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Training loop\n",
    "def train_model(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    return epoch_loss\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(preds.cpu().numpy())\n",
    "    return y_true, y_pred\n",
    "\n",
    "# Train the model for 20 epochs\n",
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train_model(model, train_loader, criterion, optimizer, device)\n",
    "    print(f'Epoch {epoch+1}/{epochs}, Loss: {train_loss:.4f}')\n",
    "\n",
    "# Evaluate the model\n",
    "y_true, y_pred = evaluate_model(model, test_loader, device)\n",
    "\n",
    "# Create a mapping from label indices to label names for the test data only\n",
    "test_class_names = label_encoder.inverse_transform(np.unique(y_test.cpu().numpy()))\n",
    "\n",
    "# Generate classification report with dynamically matched target names\n",
    "print(\"Transformer Classification Report:\\n\", classification_report(y_true, y_pred, target_names=test_class_names))\n",
    "print(\"Transformer Confusion Matrix:\\n\", confusion_matrix(y_true, y_pred))\n",
    "\n",
    "# Visualize the confusion matrix with the correct labels\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(confusion_matrix(y_true, y_pred), annot=True, fmt='d', cmap='Blues', xticklabels=test_class_names, yticklabels=test_class_names)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fall",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
